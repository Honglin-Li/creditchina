{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7fa0eda-4d24-4bbf-a209-a40b9d3f5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main_page_parser_v2\n",
    "\n",
    "The module extract data from the main page of the credit report.\n",
    "\"\"\"\n",
    "\n",
    "from setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78cfc0",
   "metadata": {},
   "source": [
    "path = r'X:/SCS/Monthers_v2.0_correct_one - Kopie/122.pdf'\n",
    "parsing_iterator(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bdc32a20-3da2-4d2d-8394-b29fe25bae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_page(path_to_pdf):\n",
    "    \"\"\"\n",
    "    Extract the data on the cover page(basic info).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_pdf : str\n",
    "        The path to the pdf to be extracted.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    tables = read_pdf(path_to_pdf, pages=2, lattice=True, pandas_options={'header': None}, silent=True)\n",
    "    \n",
    "    if len(tables) == 1: # 杭州明州医院\n",
    "        tab0 = tables[0]\n",
    "        tab1 = read_pdf(path_to_pdf, pages=3, lattice=True, pandas_options={'header': None}, silent=True)[1]\n",
    "    #if len(tables) <= 4: this cannot be a condition\n",
    "    elif tables[0].shape[1] > 3:\n",
    "        tab0 = tables[0]\n",
    "        tab1 = tables[1]\n",
    "    else:\n",
    "        tab0 = tables[1] # 0 light. basic info\n",
    "        tab1 = tables[2] # stat\n",
    "    \n",
    "    rows = tab0.shape[0]\n",
    "    temp0 = pd.concat([tab0.iloc[0:rows,0:2], tab0.iloc[0:rows-1,2:4].rename(columns={2: 0, 3: 1})], axis=0, ignore_index=True).transpose()\n",
    "    df0 = temp0.rename(columns=temp0.iloc[0]).iloc[1:]\n",
    "    \n",
    "    temp1 = pd.concat([tab1.iloc[0:4,0:2], tab1.iloc[0:4,2:4].rename(columns={2: 0, 3: 1})], axis=0, ignore_index=True).transpose()\n",
    "    df1 = temp1.rename(columns=temp1.iloc[0]).iloc[1:]\n",
    "    \n",
    "    df = pd.concat([df0, df1], axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de98c11b-6a04-41b1-b07c-e533676c1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traffic_light(path_to_pdf):\n",
    "    \"\"\"\n",
    "    Extract the credit category.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_pdf : str\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    credit_category : str\n",
    "    company_name : str\n",
    "    \"\"\"\n",
    "    traffic_dict = {}\n",
    "    \n",
    "    temp = extract_text(path_to_pdf, page_numbers=[2]).splitlines()\n",
    "    temp = [i for i in temp if i != '']\n",
    "    \n",
    "    # extract company name:\n",
    "    try:\n",
    "        company_name = temp[temp.index('企业名称：') + 1] # hidden text, not show in the pdf\n",
    "    except:\n",
    "        company_name = extract_institution_name(path_to_pdf)\n",
    "        # record the company\n",
    "        with open(r'X:/SCS/04_Results/disregistered.txt', 'a', encoding='utf-8') as disregiestered:\n",
    "            disregiestered.write(\"%s\\n\" % str(company_name))\n",
    "    \n",
    "    # traffic_light\n",
    "    if '存续' in temp:\n",
    "        traffic_dict['green'] = 1\n",
    "    else:\n",
    "        traffic_dict['green'] = 0\n",
    "        \n",
    "    if '守信激励对象' in temp:\n",
    "        traffic_dict['red'] = 1\n",
    "    else:\n",
    "        traffic_dict['red'] = 0\n",
    "        \n",
    "    if '失信惩戒对象' in temp:\n",
    "        traffic_dict['black'] = 1\n",
    "    else:\n",
    "        traffic_dict['black'] = 0\n",
    "        \n",
    "    if '注销' in temp:\n",
    "        traffic_dict['grey'] = 1\n",
    "    else:\n",
    "        traffic_dict['grey'] = 0\n",
    "    \n",
    "    return(traffic_dict, company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c441cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_institution_name(path_to_pdf):\n",
    "    \"\"\"\n",
    "    Extract the company name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_pdf : str\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "    \"\"\"\n",
    "    all_text = extract_text(path_to_pdf, page_numbers=[0]).splitlines()\n",
    "    # remove empty strings\n",
    "    all_text = [text for text in all_text if text != '']\n",
    "    \n",
    "    # name is in between 机构名称 (name) and 统一社会信用代码 (usci)\n",
    "    index_name = [index for index, text in enumerate(all_text) if '机构名称' in text][0]\n",
    "    index_usci = [index for index, text in enumerate(all_text) if '统一社会信用代码' in text][0]\n",
    "    \n",
    "    # extract the name. join used here because if name is too long, it will be written in more than 1 line.\n",
    "    name = ''.join(all_text[index_name + 1:index_usci])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4858a0c5-251b-42cc-8779-421eadc20906",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = ['行政管理',\n",
    "              '严重失信主体名单',\n",
    "              '信用承诺',\n",
    "              '司法判决',\n",
    "              '诚实守信',\n",
    "              '经营异常',\n",
    "              '信用评价',\n",
    "              '其他']\n",
    "def parsing_iterator(path_to_pdf):\n",
    "    \"\"\"\n",
    "    Extract data for one company.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_pdf : str\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    title_page = extract_title_page(path_to_pdf)\n",
    "    title_page.set_index('统一社会信用代码', inplace=True)\n",
    "    \n",
    "    # extract traffic light\n",
    "    traffic_light, company_name = extract_traffic_light(path_to_pdf)\n",
    "    \n",
    "    #title_page['机构名称'] = extract_institution_name(path_to_pdf)\n",
    "    title_page['机构名称'] = company_name\n",
    "    \n",
    "    # add name and re-order column names\n",
    "    new_order = ['机构名称']\n",
    "    new_order.extend([c for c in title_page.columns.values if c != '机构名称'])\n",
    "    title_page = title_page[new_order]\n",
    "    \n",
    "    traffic_light = pd.DataFrame.from_records(traffic_light, index=title_page.index)\n",
    "       \n",
    "    # merge them together\n",
    "    row = pd.concat([title_page, traffic_light], axis=1)\n",
    "    \n",
    "    # remove the 条\n",
    "    for event in event_dict:\n",
    "        row[event] = pd.Series([int(re.sub(',', '', re.sub('条', '', text))) for text in row[event].values]).values\n",
    "    \n",
    "    \n",
    "    row['event_count'] = row.loc[:,[col for col in row.columns if col in event_dict]].sum(axis=1)\n",
    "    row['n_pages'] = len(list(extract_pages(path_to_pdf)))\n",
    "    row['file_path'] = path_to_pdf\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40786673-78e5-4caa-ae76-4b8e44fdd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which iterates over all documents and returns dataframe with main page information\n",
    "\n",
    "\n",
    "def create_main_df(pdf_paths):\n",
    "    \"\"\"\n",
    "    Extract data on cover page for list of pdf paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_paths : list of str\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrmae\n",
    "    \"\"\"\n",
    "    df = pd.concat([parsing_iterator(pdf) for pdf in tqdm(pdf_paths, position=0, leave=True)])\n",
    "    #Drop duplicate companies if there are any\n",
    "    df = df.reset_index()#.drop_duplicates(subset='统一社会信用代码').set_index('统一社会信用代码', drop=True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4599e",
   "metadata": {},
   "source": [
    "path_to_pdf = r'X://SCS//Daughters-Duplicated//72226.pdf'\n",
    "tables = read_pdf(path_to_pdf, pages=2, lattice=True, pandas_options={'header': None}, silent=True)\n",
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480d159",
   "metadata": {},
   "source": [
    "# special companies\n",
    "create_main_df([r'X://SCS//Daughters-Duplicated//72226.pdf', r'X://SCS//Daughters-Duplicated//21057.pdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faceee9",
   "metadata": {},
   "source": [
    "extract_title_page(path_to_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888da35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
