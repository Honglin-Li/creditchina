{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207da057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npipeline\\n\\nProvide all the functions to create the final dataset & Internal consistency check.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pipeline\n",
    "\n",
    "Provide all the functions to create the final dataset & Internal consistency check.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70539735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for jupyter notebook\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parents[0].parents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e0e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_transformation.utils import *\n",
    "import src.data_transformation.event1_permit_processing as event1_permit\n",
    "import src.data_transformation.event1_penalty_processing as event1_penalty\n",
    "import src.data_transformation.event2_processing as event2\n",
    "import src.data_transformation.event3_processing as event3\n",
    "import src.data_transformation.event4_processing as event4\n",
    "import src.data_transformation.event5_processing as event5\n",
    "from src.data_transformation.split_events import split_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b06395",
   "metadata": {},
   "source": [
    "# Credit dataset set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f6f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crosstab_dataframes(file_name_list, func_list):\n",
    "    \"\"\"\n",
    "    Get crosstab(year) and day-level DataFrames.\n",
    "    \n",
    "    This func wont run by calling get_meta_year_table. If you want to re-create those crosstabs for events, \\\n",
    "    you need to delete the files in local first.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name_list : list of str\n",
    "        The item is the file name without extension, like event1.\n",
    "    func_list : list of functions\n",
    "        The func is to create crosstabs if the crosstabs not in local.\n",
    "    \n",
    "    Returns\n",
    "    ----\n",
    "    crosstabs : list of DataFrames\n",
    "    crosstabs_year : list of DataFrames\n",
    "    crosstabs_day : list of DataFrames\n",
    "    \"\"\"   \n",
    "    crosstabs = []\n",
    "    crosstabs_year = []\n",
    "    crosstabs_day = []\n",
    "    \n",
    "    for file, func in zip(file_name_list, func_list):\n",
    "        # get file name of crosstabs\n",
    "        crosstab_path, crosstab_year_path, crosstab_day_path = get_meta_event_paths(file)\n",
    "        \n",
    "        # not exists -> create\n",
    "        if not os.path.exists(crosstab_day_path):\n",
    "            # call responding function to generate\n",
    "            print(f'create crosstabs for {file}...')\n",
    "            df_crosstab, df_crosstab_year, df_crosstab_day = func()\n",
    "            \n",
    "        else:\n",
    "            df_crosstab = pd.read_excel(crosstab_path, index_col=[0, 1])\n",
    "            df_crosstab_year = pd.read_excel(crosstab_year_path, index_col=[0, 1, 2])\n",
    "            df_crosstab_day = pd.read_excel(crosstab_day_path, index_col=[0, 1, 2])\n",
    "        \n",
    "        crosstabs.append(df_crosstab)\n",
    "        crosstabs_year.append(df_crosstab_year)\n",
    "        crosstabs_day.append(df_crosstab_day)\n",
    "    \n",
    "    return crosstabs, crosstabs_year, crosstabs_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c06d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_dataframes(file_name_list):\n",
    "    \"\"\"\n",
    "    Get list of processed event DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name_list : list of str\n",
    "        The item is the file name without extension, like event1.\n",
    "    \n",
    "    Returns\n",
    "    ----\n",
    "    list of DataFrames\n",
    "        The item is processed sub-event DataFrame.\n",
    "    \"\"\"   \n",
    "    results = []\n",
    "    \n",
    "    for file in file_name_list:\n",
    "        path = os.path.join(processed_event_path, file + '.xlsx')\n",
    "        \n",
    "        # check exists\n",
    "        if not os.path.exists(path):\n",
    "            print(f'{file} does not exists, you can call function get_crosstab_dataframes() to create processed events and crosstabs.')\n",
    "            break\n",
    "            \n",
    "        results.append(\n",
    "            pd.read_excel(path)\n",
    "        )\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead8d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_event_numbers(df):\n",
    "    \"\"\"\n",
    "    Add total event numbers for meta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    df['event_number'] = df[[\n",
    "        'permit',\n",
    "        'penalty',\n",
    "        'redlist', \n",
    "        'blacklist', \n",
    "        'watchlist',\n",
    "        'commit'\n",
    "    ]].sum(axis=1) \n",
    "    \n",
    "    df = move_last_column_to_first(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06476695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_companies(df_meta, metadata, data_type):\n",
    "    \"\"\"\n",
    "    Assign red/black/green to each company each record of DataFrame df_meta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The DataFrame can be crosstab_year or crosstab_day.\n",
    "    metadata : DataFrame\n",
    "        The main metadata which contains company_name, ID, and foundation_year.\n",
    "    date_type : str\n",
    "        Can be 'year' or 'day'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        With additional red, black, green columns.\n",
    "    \"\"\"\n",
    "    # 1. Label\n",
    "    def label_black_red(df_meta):\n",
    "        # set black. RULE: in blacklists\n",
    "        df_meta['black'] = (df_meta.blacklist > 0).astype(int)\n",
    "\n",
    "        # set red. RULE: in redlists & not in blacklists\n",
    "        df_meta['red'] = ((df_meta.redlist > 0) & (df_meta.black == 0)).astype(int)\n",
    "    \n",
    "    label_black_red(df_meta)\n",
    "    \n",
    "    if data_type == 'year':\n",
    "        df_meta = handle_green_credit_rating(df_meta, metadata)\n",
    "        \n",
    "    else:\n",
    "        # for metadata_day, only add green label\n",
    "        df_meta['green'] = 0\n",
    "        \n",
    "    # assign green to NA\n",
    "    green_idx = df_meta[(df_meta['red'] == 0) & (df_meta['black'] == 0)].index\n",
    "    df_meta.loc[green_idx, 'green'] = 1\n",
    "    df_meta['green'] = df_meta['green'].fillna(0)\n",
    "    \n",
    "    label_black_red(df_meta) # in case sth wrong, label one more time\n",
    "    \n",
    "    # 2. Check the correctness of credit category\n",
    "    def show_incorrect_credit_category(color, incorrect_idx):\n",
    "        print(f'check credit category: {color}...')\n",
    "        print(incorrect_idx.sum())\n",
    "        \n",
    "        if incorrect_idx.sum() > 0:\n",
    "            print(df_meta[incorrect_idx].head(50))\n",
    "        \n",
    "    incorrect_idx = (df_meta.blacklist > 0) & (df_meta.black != 1)\n",
    "    show_incorrect_credit_category('black', incorrect_idx)\n",
    "    \n",
    "    incorrect_idx = (df_meta.blacklist == 0) & (df_meta.redlist > 0) & (df_meta.red != 1)\n",
    "    show_incorrect_credit_category('red', incorrect_idx)\n",
    "    \n",
    "    incorrect_idx = (df_meta.blacklist == 0) & (df_meta.redlist == 0) & (df_meta.green != 1)\n",
    "    show_incorrect_credit_category('green', incorrect_idx)\n",
    "    \n",
    "    print('check if green+black+red=1...')\n",
    "    print(df_meta[['black', 'red', 'green']].sum(axis=1).value_counts())\n",
    "    \n",
    "    return df_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d301c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_green_credit_rating(df_meta, metadata):\n",
    "    \"\"\"\n",
    "    Assign green rating for all companies all years (new records are added).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The DataFrame can be crosstab_year or crosstab_day.\n",
    "    metadata : DataFrame\n",
    "        The main metadata which contains company_name, ID, and foundation_year.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        With additional green column and rows (companies without any event records).\n",
    "    \"\"\"\n",
    "    df_source = metadata.reset_index()[['ID', 'company_name', 'foundation_year']]\n",
    "    \n",
    "    # Type1: companies without any event records, so that not in df_meta\n",
    "    companies = list(set(df_source.company_name.unique()) - set(df_meta.index.unique('company_name')))\n",
    "    \n",
    "    print(f'TYPE1: {len(companies)} companies do not have any event records.')\n",
    "    \n",
    "    df_new_companies = df_source[df_source['company_name'].isin(companies)]\n",
    "    \n",
    "    # handle the year\n",
    "    df_new_companies = prepare_year_column(df_new_companies, 'ID', 'foundation_year') # columns: company_name, ID, year\n",
    "    \n",
    "    # Type2: companies in df_meta but the start year is not equal to the foundation year\n",
    "    df_exist_companies = df_meta.reset_index().groupby('company_name')['year'].min()\n",
    "    df_exist_companies = df_exist_companies.to_frame().join(df_source.set_index('company_name'), how='left') # add foundation year\n",
    "    df_exist_companies['end_year'] = df_exist_companies.year - 1\n",
    "    \n",
    "    # remove the companies with start year equal to 2014\n",
    "    df_exist_companies = df_exist_companies[df_exist_companies.year > s_year].reset_index()\n",
    "    \n",
    "    # handle the year\n",
    "    df_exist_companies = prepare_year_column(df_exist_companies, 'ID', 'foundation_year', 'end_year')\n",
    "    \n",
    "    # Type3: companies in df_meta but the end year is not equal to 2022 (not have to be green)\n",
    "    df_exist_companies_end = df_meta.reset_index().groupby('company_name')['year'].max().to_frame()\n",
    "    \n",
    "    df_exist_companies_end = df_exist_companies_end[df_exist_companies_end.year < e_year].reset_index()\n",
    "    \n",
    "    df_exist_companies_end['start_year'] = df_exist_companies_end.year + 1\n",
    "    \n",
    "    # combine the 2 types of companies\n",
    "    df_new_records = pd.concat([df_new_companies, df_exist_companies])\n",
    "    \n",
    "    # add credit rating columns\n",
    "    df_new_records['red'] = 0\n",
    "    df_new_records['black'] = 0\n",
    "    df_new_records['green'] = 1\n",
    "    \n",
    "    if df_exist_companies_end.shape[0] > 0:\n",
    "        # handle the year\n",
    "        df_exist_companies_end = prepare_year_column(assign_id_to_company(df_exist_companies_end), 'ID', 'start_year')\n",
    "        \n",
    "        # combine the type 3\n",
    "        df_new_records = pd.concat([df_new_records, df_exist_companies_end]) # credit category columns will be NA\n",
    "    \n",
    "    df_new_records = df_new_records.set_index(['ID', 'company_name', 'year'])\n",
    "    \n",
    "    df_meta = pd.concat([df_meta.reset_index(), df_new_records.reset_index()]) # concat new rows\n",
    "    \n",
    "    # remove the incorrect observations before foundation date.\n",
    "    df_meta = df_meta.reset_index(drop=True).merge(df_source[['ID', 'foundation_year']], how='left', on='ID') # add foundation year\n",
    "    \n",
    "    remove_idx = df_meta[df_meta.year < df_meta.foundation_year].index\n",
    "    print(f'{len(remove_idx)} observations are earlier than foundation year')\n",
    "    \n",
    "    df_meta = df_meta.drop(\n",
    "        remove_idx\n",
    "    ).drop(\n",
    "        ['foundation_year'], \n",
    "        axis=1\n",
    "    ).sort_values(\n",
    "        ['ID', 'year']\n",
    "    ).set_index(\n",
    "        ['ID', 'company_name', 'year']\n",
    "    )\n",
    "    \n",
    "    # handle Type3's credit category\n",
    "    df_meta[['red', 'black']] = df_meta[['red', 'black']].fillna(method='ffill') # green can not fill NA\n",
    "    \n",
    "    return df_meta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab8614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_table(crosstabs=None, new=False):\n",
    "    \"\"\"\n",
    "    Load from local or Create Metadata by merging descriptive columns in df_meta and crosstabs from events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    crosstabs : list of DataFrame, optinal\n",
    "        The param is used to create new metadata.\n",
    "        The item is cross table year from each event. \n",
    "        If there is metadata local, do not need the parameter.\n",
    "    new : bool, default False\n",
    "        If True -> even metadata file in local, still create a new one.\n",
    "        When it is True, crosstabs cannot be None.\n",
    "        If False -> if file in local, just load.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    original_meta_path = os.path.join(sub_event_path, 'metadata.xlsx')\n",
    "    processed_meta_path = os.path.join(processed_data_path, 'metadata.xlsx')\n",
    "    \n",
    "    if os.path.exists(processed_meta_path) & ~new:\n",
    "        # load from local\n",
    "        print('load meta data...')\n",
    "        df_meta = pd.read_excel(processed_meta_path, index_col=[0, 1])\n",
    "        return df_meta\n",
    "    \n",
    "    print('create meta data...')\n",
    "    \n",
    "    # load original df_meta, do not use those number columns\n",
    "    print('1. load data...')\n",
    "    df_meta = pd.read_excel(original_meta_path)\n",
    "    \n",
    "    # fill out the missing 40 region info\n",
    "    # get the 40 records\n",
    "    if df_meta[df_meta.level=='uncertain'].shape[0] > 0:\n",
    "        print('fill out the missing region info')\n",
    "\n",
    "        df_region = pd.read_excel(os.path.join(attri_path, 'companies_without_region.xlsx'))\n",
    "\n",
    "        for i, row in df_region.iterrows():\n",
    "            # get region values\n",
    "            level = row['level']\n",
    "            region = row['region']\n",
    "            province = row['province']\n",
    "\n",
    "            # fill out value\n",
    "            row_index = df_meta[df_meta[company_name]==row['company_name']].index[0]\n",
    "\n",
    "            df_meta.at[row_index, 'level'] = level\n",
    "            df_meta.at[row_index, 'region'] = region\n",
    "            df_meta.at[row_index, 'province'] = province\n",
    "\n",
    "        # save meta\n",
    "        df_meta.to_excel(original_meta_path, index=False, freeze_panes=(1, 3))\n",
    "    \n",
    "    df_meta = df_meta[[\n",
    "        'ID', company_name,\n",
    "        'level', 'region', 'province', 'M_D', # TODO: daughter corresponding\n",
    "        '企业类型 Corporate type',\n",
    "        '成立日期 Date of Foundation', 'foundation_year',\n",
    "        'red', 'black', 'green', 'grey',\n",
    "        'Latest evaluation year for A taxpayer'\n",
    "    ]].rename(columns={\n",
    "        company_name: 'company_name',\n",
    "        '成立日期 Date of Foundation': 'foundation_date',\n",
    "        '企业类型 Corporate type': 'corporate_type'\n",
    "    }).set_index(['ID', 'company_name'])\n",
    "    \n",
    "    # merge crosstabs\n",
    "    print('2. merge with crosstabs...')\n",
    "    df_crosstabs = pd.concat(crosstabs, axis=1)\n",
    "    \n",
    "    # add margins\n",
    "    df_crosstabs = add_total_event_numbers(df_crosstabs)\n",
    "    \n",
    "    # merge meta and events stat crosstabs\n",
    "    df_meta = df_meta.merge(df_crosstabs, how='left', on=['ID', 'company_name'])\n",
    "    \n",
    "    print('3. save metadata...')\n",
    "    df_meta.to_excel(processed_meta_path, freeze_panes=(1, 2))\n",
    "    \n",
    "    return df_meta    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f279c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_period_table(metadata, crosstabs, date_type, new):\n",
    "    \"\"\"\n",
    "    Merge crosstabs(year or day level), then label, save to local as well as the credit category simplier version.\n",
    "    The code is extracted from the common code in functions get_meta_year_table() and get_meta_day_table().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : DataFrame\n",
    "        The main metadata which contains company_name, ID, and foundation_year.\n",
    "    crosstabs : list of DataFrame\n",
    "    date_type : str\n",
    "        Can be 'year' or 'day'\n",
    "    new : bool\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    meta_file_name = 'panel_data_' + date_type + '.xlsx'\n",
    "    flag_file_name = 'panel_data_' + date_type + '_simple.xlsx'\n",
    "    demo_file_name = 'panel_data_' + date_type + '_demo.xlsx'\n",
    "    demo_flag_file_name = 'panel_data_' + date_type + '_simple_demo.xlsx'\n",
    "    \n",
    "    processed_meta_path = os.path.join(processed_data_path, meta_file_name)\n",
    "    \n",
    "    if os.path.exists(processed_meta_path) & ~new:\n",
    "        # load from local\n",
    "        df_meta = pd.read_excel(processed_meta_path, index_col=[0, 1, 2])\n",
    "        return df_meta\n",
    "    \n",
    "    print('create meta data ')\n",
    "    \n",
    "    # merge crosstabs\n",
    "    print('1. merge crosstabs...')\n",
    "    df_meta = pd.concat(crosstabs, axis=1)\n",
    "    \n",
    "    # add margins \n",
    "    df_meta = add_total_event_numbers(df_meta)\n",
    "    \n",
    "    # for day panel, remove the date after e_date\n",
    "    if date_type == 'day':\n",
    "        df_meta = df_meta.drop(df_meta[df_meta.index.get_level_values('date').str[:4].astype(int) > e_year].index)\n",
    "        \n",
    "        # sort\n",
    "        df_meta = df_meta.sort_index(level=['ID', 'date'])\n",
    "        \n",
    "        # fill the empty dates\n",
    "        df_meta = df_meta.groupby(level='ID', as_index=False).ffill()\n",
    "    \n",
    "    # label red black \n",
    "    print('2. add credit rating columns...')\n",
    "    df_meta = label_companies(df_meta, metadata, date_type)\n",
    "\n",
    "    df_meta = df_meta.pipe(move_last_column_to_first).pipe(move_last_column_to_first).pipe(move_last_column_to_first)\n",
    "\n",
    "    # handle missing years in the middle\n",
    "    if date_type == 'year':\n",
    "        df_meta = complete_missing_years(df_meta)\n",
    "    \n",
    "    # save\n",
    "    print('3. save...')\n",
    "    \n",
    "    # save demo, cuz too big to browse normally\n",
    "    df_meta[:500].to_excel(os.path.join(processed_data_path, demo_file_name), freeze_panes=(1, 3))\n",
    "    \n",
    "    #if date_type == \"year\":\n",
    "    # generate flag\n",
    "    df_flag = df_meta[['green', 'red', 'black', 'redlist', 'blacklist', 'watchlist', 'permit', 'penalty', 'commit']]\n",
    "    \n",
    "    # save flag\n",
    "    df_flag[:500].to_excel(\n",
    "        os.path.join(processed_data_path, demo_flag_file_name), freeze_panes=(1, 3))\n",
    "    \n",
    "    # save the master table\n",
    "    if date_type == 'day': # too big to save in excel\n",
    "        flag_file_name = 'panel_data_' + date_type + '_simple.dta'\n",
    "        \n",
    "        df_flag.to_stata(\n",
    "            os.path.join(processed_data_path, flag_file_name), version=118)\n",
    "        \n",
    "        meta_file_name = 'panel_data_' + date_type + '.dta'\n",
    "        \n",
    "        df_meta.to_stata(os.path.join(processed_data_path, meta_file_name), version=118)\n",
    "        \n",
    "        return df_meta\n",
    "    \n",
    "    # save normally\n",
    "    df_flag.to_excel(\n",
    "        os.path.join(processed_data_path, flag_file_name), freeze_panes=(1, 3))\n",
    "\n",
    "    df_meta.to_excel(processed_meta_path, freeze_panes=(1, 3))\n",
    "\n",
    "    return df_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7887df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_year_table(metadata, crosstabs=None, new=False):\n",
    "    \"\"\"\n",
    "    Load or Create panel_data_year by merge crosstabs_year from events(with red black labels).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : DataFrame\n",
    "        The main metadata which contains company_name, ID, and foundation_year.\n",
    "    crosstabs : list of DataFrame, optinal\n",
    "        The param is used to create new metadata.\n",
    "        The item is cross table year from each event. \n",
    "        If there is metadata local, do not need the parameter.\n",
    "    new : bool\n",
    "        If True -> even metadata file in local, still create a new one.\n",
    "        When it is True, crosstabs cannot be None.\n",
    "        If False -> if file in local, just load.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"      \n",
    "    print('handle year-level metadata...')\n",
    "    \n",
    "    df_meta_year = get_meta_period_table(\n",
    "        metadata,\n",
    "        crosstabs, \n",
    "        'year',\n",
    "        new)\n",
    "    \n",
    "    return df_meta_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a86463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_years(df_meta_year):\n",
    "    \"\"\"\n",
    "    Somehow some companies' years are incomplete.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta_year : DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"    \n",
    "    def complete_year(group):\n",
    "        # generate the right years\n",
    "        first_year = group.year.tolist()[0]\n",
    "        last_year = group.year.tolist()[-1]\n",
    "\n",
    "        consecutive_years = set(range(first_year, last_year+1))\n",
    "\n",
    "        actual_years = set(group.year)\n",
    "\n",
    "        if len(consecutive_years) == len(actual_years):\n",
    "            return group\n",
    "\n",
    "        missing_years = list(consecutive_years.difference(actual_years))\n",
    "\n",
    "        # add missing years\n",
    "        for year in missing_years:\n",
    "            group = group.append([{'year': year}], ignore_index=True)\n",
    "\n",
    "        # order and fillna\n",
    "        return group.sort_values('year').ffill()\n",
    "\n",
    "    return df_meta_year.reset_index().groupby(\n",
    "        ['ID']\n",
    "    ).apply(complete_year).set_index(['ID', 'company_name', 'year'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "561620f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_day_table(metadata, crosstabs=None, new=False):\n",
    "    \"\"\"\n",
    "    Get panel_data_day by merge crosstabs_day from events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : DataFrame\n",
    "        The main metadata which contains company_name, ID, and foundation_year.\n",
    "    crosstabs : list of DataFrame, optinal\n",
    "        The param is used to create new metadata.\n",
    "        If there is metadata local, do not need the parameter.\n",
    "    new : bool\n",
    "        If True -> even metadata file in local, still create a new one.\n",
    "        When it is True, crosstabs cannot be None.\n",
    "        If False -> if file in local, just load.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    print('handle day-level metadata...')\n",
    "    \n",
    "    df_meta_day = get_meta_period_table(\n",
    "            metadata,\n",
    "            crosstabs, \n",
    "            'day',\n",
    "            new)\n",
    "\n",
    "    return df_meta_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8a30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_M_D_column(df, df_meta):\n",
    "    \"\"\"\n",
    "    Add M_D column from df_meta to df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The DataFrame needs to be added M_D column.\n",
    "    df_meta : DataFrame\n",
    "        The metadata with M_D column. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A DataFrame with added M_D column.\n",
    "    \"\"\" \n",
    "    index_names = df.index.names\n",
    "    has_multi_index = 'company_name' in index_names\n",
    "    \n",
    "    df_left = df.copy()\n",
    "    \n",
    "    if has_multi_index:\n",
    "        df_left = df_left.reset_index()\n",
    "        \n",
    "    df_left = df_left.merge(\n",
    "        df_meta.reset_index()[['company_name', 'M_D']], \n",
    "        how='left',\n",
    "        on='company_name')\n",
    "    \n",
    "    if has_multi_index:\n",
    "        df_left = df_left.set_index(index_names)\n",
    "        \n",
    "    return df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d845ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_M_D_column_for_list(df_list, df_meta):\n",
    "    \"\"\"\n",
    "    Add M_D column from df_meta to a list of DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_list : list of DataFrame\n",
    "        The DataFrame needs to be added M_D column.\n",
    "    df_meta : DataFrame\n",
    "        The metadata with M_D column. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of DataFrame\n",
    "    \"\"\" \n",
    "    return [df.pipe(add_M_D_column, df_meta) for df in df_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa20ca",
   "metadata": {},
   "source": [
    "# Credit category variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac52ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_credit_category_column(df_meta):\n",
    "    \"\"\"\n",
    "    Combine credit category to one color column and Return a DataFrame with added new color column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The DataFrame with at least columns: company_name, green, red, and black.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    # combine green red black to 1 credit rating columns\n",
    "    df_meta['color'] = df_meta.apply(\n",
    "        lambda row: 'green' if row['green'] > 0 else ('red' if row['red'] > 0 else 'black')\n",
    "        , axis=1\n",
    "    )\n",
    "\n",
    "    return df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab0a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rating_variation_types(df_meta_year=None):\n",
    "    \"\"\"\n",
    "    Output what kinds of credit category variation types the dataset have.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta_year : DataFrame, default None\n",
    "        The panel_data_year(_simple) DataFrame. If it is None, import from local.\n",
    "    \"\"\"   \n",
    "    if df_meta_year is None:\n",
    "        df_meta_year = get_credit_panel_data()\n",
    "\n",
    "    df_rating_change = add_credit_category_column(df_meta_year).reset_index()[['company_name', 'color']]\n",
    "\n",
    "    # remove the consecutive duplicates\n",
    "    df_rating_change = df_rating_change.loc[df_rating_change.color.shift() != df_rating_change.color]\n",
    "\n",
    "    # remove the companies without variation (only one record)\n",
    "    df_rating_change = df_rating_change[df_rating_change.company_name.duplicated(keep=False)]\n",
    "\n",
    "    # new column to capture the variation\n",
    "    df_rating_change['rating_change'] = df_rating_change.groupby('company_name')['color'].transform(lambda x: '->'.join(x))\n",
    "\n",
    "    print(df_rating_change.head(10))\n",
    "\n",
    "    print('The dataset exists the following credit rating changing types.')\n",
    "    print(df_rating_change.rating_change.unique())\n",
    "    # out: ['green->red' 'green->black' 'green->red->black' 'red->black']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf26d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markov_data(df_meta_year):\n",
    "    \"\"\"\n",
    "    Output the percentage of the state transition of credit category.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta_year : DataFrame, default None\n",
    "        The panel_data_year(_simple) DataFrame. If it is None, import from local.\n",
    "    \"\"\"  \n",
    "    if df_meta_year is None:\n",
    "        df_meta_year = get_credit_panel_data()\n",
    "\n",
    "    df_color = add_credit_category_column(df_meta_year).reset_index()[['company_name', 'color']]\n",
    "    \n",
    "    # add next color\n",
    "    df_color['color_next'] = df_color.color.shift(-1)\n",
    "    \n",
    "    # remove last row in each company group (cuz no need)\n",
    "    df_color = df_color.groupby(\n",
    "        'company_name', as_index=False\n",
    "    ).apply(\n",
    "        lambda group: group.iloc[:-1]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # add color transition column \n",
    "    df_color['transition'] = df_color.color + '->' +  df_color.color_next\n",
    "    \n",
    "    # show counts, probability can be calculated self\n",
    "    print(df_color.transition.value_counts().sort_index().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1679312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_variation_table(df_meta_year):\n",
    "    \"\"\"\n",
    "    Return the credit category variation data by year, which provide data for credit category change chart.\n",
    "    \n",
    "    If a separate stat of mother and daughter is needed, should import df_meta_year with only mother or daughter companies.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta_year : DataFrame\n",
    "        The panel_data_year(_simple) DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The DataFrame contains green red black firms and their percentages by year.\n",
    "    \"\"\" \n",
    "    # rating variations\n",
    "    # color by year\n",
    "    df_rating_change = df_meta_year.reset_index().groupby('year')[['green', 'red', 'black']].sum()\n",
    "\n",
    "    # total firms \n",
    "    df_rating_change['firms'] = df_rating_change.red + df_rating_change.black + df_rating_change.green\n",
    "\n",
    "    # add pct\n",
    "    get_pct = lambda color: np.round(df_rating_change[color] / df_rating_change.firms * 100, 2)\n",
    "\n",
    "    df_rating_change['green_pct'] = get_pct('green')\n",
    "    df_rating_change['red_pct'] = get_pct('red')\n",
    "    df_rating_change['black_pct'] = get_pct('black')\n",
    "    \n",
    "    return df_rating_change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bc314d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_year_for_variation_firms(df_meta_year=None):\n",
    "    \"\"\"\n",
    "    Create and save the variation(credit ratings) frims in df_meta_year.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta_year : DataFrame, default None\n",
    "        The panel_data_year(_simple) DataFrame. If it is None, import from local.\n",
    "    \"\"\"   \n",
    "    path = os.path.join(processed_data_path, 'panel_data_year_credit_category_variation.xlsx')\n",
    "    \n",
    "    print('create the crosstable for variation firms...')\n",
    "    \n",
    "    # get crosstab_year table\n",
    "    if df_meta_year is None:\n",
    "        df_meta_year = pd.read_excel(os.path.join(processed_data_path, 'panel_data_year_simple.xlsx'), index_col=[0,1,2])\n",
    "    \n",
    "    time_df = df_meta_year.reset_index()\n",
    "\n",
    "    # create r b g count table\n",
    "    df_variation = time_df.groupby('company_name', as_index=False)['red', 'black', 'green'].sum()\n",
    "    df_variation[['red', 'black', 'green']] = df_variation[['red', 'black', 'green']].astype('bool').astype('int')\n",
    "    \n",
    "    def get_data_from_mask(mask):\n",
    "        variation_firms = df_variation[mask].company_name.values\n",
    "        print(f'company count: {len(variation_firms)}')\n",
    "        \n",
    "        if len(variation_firms) == 0:\n",
    "            return None\n",
    "        \n",
    "        return time_df[time_df.company_name.isin(variation_firms)].set_index(['ID', 'company_name', 'year'])\n",
    "    \n",
    "    with pd.ExcelWriter(path) as writer:\n",
    "        print('Type: red -> black') # 1\n",
    "        df_rb = get_data_from_mask(((df_variation.red + df_variation.black) == 2) & (df_variation.green == 0))\n",
    "        if df_rb is not None:\n",
    "            df_rb.to_excel(writer, sheet_name='red_to_black', freeze_panes=(1,3))\n",
    "        \n",
    "        print('Type: green -> red -> black') # 661\n",
    "        df_grb = get_data_from_mask((df_variation.red + df_variation.black + df_variation.green) == 3)\n",
    "        df_grb.to_excel(writer, sheet_name='green_red_black', freeze_panes=(1,3))\n",
    "        \n",
    "        print('Type: green -> black') # 2302\n",
    "        df_gb = get_data_from_mask((df_variation.black + df_variation.green) == 2)\n",
    "        df_gb.to_excel(writer, sheet_name='green_black', freeze_panes=(1,3))\n",
    "        \n",
    "        print('Type: green -> red') # 54213\n",
    "        df_gr = get_data_from_mask(((df_variation.red + df_variation.green) == 2) & (df_variation.black == 0))\n",
    "        df_gr.to_excel(writer, sheet_name='green_red', freeze_panes=(1,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267c513",
   "metadata": {},
   "source": [
    "# Internal consistency check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ccd7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_consistency_check_year(df_meta, df_meta_year):\n",
    "    \"\"\"\n",
    "    Internal consistency check: if the firm's fo\n",
    "    \n",
    "    undation year is equal to the earliest year in panel_data_year.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The metadata DataFrame.\n",
    "    df_meta_year : DataFrame\n",
    "        The panel_data_year(_simple) DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The DataFrame contains green red black firms and their percentages by year.\n",
    "    \"\"\" \n",
    "    # internal consistent check\n",
    "    # 1. check foundation year and the earliest year in df_meta_year\n",
    "    # get the earliest year\n",
    "    df_check = df_meta_year.reset_index().groupby('company_name', as_index=False)['year'].min() \n",
    "\n",
    "    # join foundation year\n",
    "    df_check = df_check.merge(\n",
    "        df_meta.reset_index()[['company_name', 'foundation_year']],\n",
    "        how='left',\n",
    "        on='company_name'\n",
    "        )\n",
    "\n",
    "    # add match flag\n",
    "    df_check['flag'] = df_check.year == df_check.foundation_year\n",
    "\n",
    "    # handle 2014\n",
    "    df_check.loc[(df_check.year >= df_check.foundation_year) & (df_check.year == 2014), 'flag'] = True\n",
    "    \n",
    "    return df_check[df_check.flag==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a7a65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_event_idx(df_meta, df_event, start_col='start_date', foundation_col='foundation_date'):\n",
    "    \"\"\"\n",
    "    Internal consistency check: return the record idx where the start year of event record is ealier than the firm's foundation year.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The metadata DataFrame.\n",
    "    df_event : DataFrame\n",
    "        The processed event DataFrame.\n",
    "    start_col : str, default 'start_date'\n",
    "        The column name of start year or date of a event.\n",
    "    foundation_col : str, default 'foundation_date'\n",
    "        Can be 'foundation_year' (for redlists)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Int64Index\n",
    "    \"\"\" \n",
    "    # join foundation year\n",
    "    df_check = df_event[['company_name', start_col]].merge(\n",
    "        df_meta.reset_index()[['company_name', 'foundation_year', 'foundation_date']],\n",
    "        how='left',\n",
    "        on='company_name'\n",
    "        ) # company_name, start_year/date, foundation_year, foundation_date\n",
    "\n",
    "    # add match flag\n",
    "    df_check['flag'] = (df_check[start_col] >= df_check[foundation_col])\n",
    "    \n",
    "    # ignore those companies without foundation year\n",
    "    df_check.loc[df_check[foundation_col].isnull(), 'flag'] = True\n",
    "    \n",
    "    df_incorrect = df_event[df_check.flag==False]\n",
    "    \n",
    "    print(f'{df_incorrect.shape[0]} incorrect records')\n",
    "    \n",
    "    return df_incorrect.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac6ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_incorrect_event_records(df_meta):\n",
    "    \"\"\"\n",
    "    Remove all events' incorect records where the start year of event record is ealier than the firm's foundation year, \n",
    "    then Save to local(clean_sub_events).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The metadata DataFrame with foundation_year and foundation_date columns.\n",
    "    \"\"\" \n",
    "    # prepare events, other sub-events have been verified and do not have incorrect records\n",
    "    l_path = [\n",
    "        '11_event_permit.xlsx',\n",
    "        '22_event_a_taxpayer.xlsx',\n",
    "        '31_event_dishonest_person.xlsx',\n",
    "        '4_event_abnormal_operations.xlsx',    \n",
    "        '51_event_commitment_implementation.xlsx'    \n",
    "    ]\n",
    "    \n",
    "    l_start = [\n",
    "        'start_year',\n",
    "        '评价年度 Evaluation year',\n",
    "        'issue_date',\n",
    "        'start_date',\n",
    "        'start_date'\n",
    "    ]\n",
    "    \n",
    "    l_foundation = [\n",
    "        'foundation_year',\n",
    "        'foundation_year',\n",
    "        'foundation_date',\n",
    "        'foundation_date',\n",
    "        'foundation_date'\n",
    "    ]\n",
    "    \n",
    "    for path, start_col, foundation_col in zip(l_path, l_start, l_foundation):\n",
    "        print(f'handle {path}...')\n",
    "        \n",
    "        # load event\n",
    "        save_path = os.path.join(sub_event_path, path)\n",
    "        df_event = pd.read_excel(save_path, dtype={start_col: str})\n",
    "        \n",
    "        # get idx for incorrect records\n",
    "        idx=get_incorrect_event_idx(df_meta, df_event, start_col, foundation_col)\n",
    "        \n",
    "        if path == '11_event_permit.xlsx':\n",
    "            # for permit, some permits are surely earlier before foundation date\n",
    "            df_incorrect = df_event[df_event.index.isin(idx)]\n",
    "            \n",
    "            # refine the index, keep permits for bank account opening and Enterprise name pre-approval\n",
    "            idx = df_incorrect[~df_incorrect['许可内容 Permission Content'].str.contains(r'名称.*核准|开户|开立', regex=True)].index\n",
    "        \n",
    "        if len(idx) > 0:\n",
    "            print(f'remove incorrect records: {len(idx)}, then save to local')\n",
    "            df_without_incorrect = df_event[~df_event.index.isin(idx)]\n",
    "            \n",
    "            # save correct records\n",
    "            df_without_incorrect.to_excel(\n",
    "                save_path, \n",
    "                index=False, \n",
    "                freeze_panes=(1, 2))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "161df891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_consistency_check_credit_category(df_meta, df_meta_period, date_type='year'):\n",
    "    \"\"\"\n",
    "    Create and save two unmatch tables(red and black) including actual and predicted credit categories of 2022.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "        The meta data contains true labels(red and black)\n",
    "    df_meta_period : DataFrame\n",
    "    date_type : str, default 'year'\n",
    "        The value can be 'year' or 'day'.\n",
    "    \"\"\"\n",
    "    # 1. generate df_compare\n",
    "    print(f'compare {date_type}-level predictions and actual red/black labels...')\n",
    "    \n",
    "    df_flag = df_meta_period[['red', 'black', 'redlist', 'blacklist', 'watchlist']]\n",
    "\n",
    "    # get predicted records\n",
    "    # only get the last record of each company\n",
    "    if date_type == 'day':\n",
    "        df_compare = df_flag.groupby(level='company_name').last()\n",
    "    else:\n",
    "        # if handle like 'day', there are some negative errors\n",
    "        df_compare = df_flag.reset_index()\n",
    "        df_compare = df_compare[df_compare.year == e_year].set_index(['company_name'])\n",
    "\n",
    "    # add actual red black by crossing merge with df_meta\n",
    "    df_compare = df_compare.merge(df_meta[['red', 'black']],\n",
    "                                 how='left',\n",
    "                                 on='company_name',\n",
    "                                 suffixes=('_predicted', '_actual'))\n",
    "    \n",
    "    # add flag to show each company is match or not\n",
    "    df_compare['red_match'] = (df_compare['red_actual'] == df_compare['red_predicted'])\n",
    "    df_compare['black_match'] = (df_compare['black_actual'] == df_compare['black_predicted'])\n",
    "\n",
    "    df_compare = df_compare[[\n",
    "        'red_actual', 'red_predicted', 'red_match',\n",
    "        'black_actual', 'black_predicted', 'black_match',\n",
    "        'redlist', 'blacklist', 'watchlist'\n",
    "    ]]\n",
    "    \n",
    "    # 2. Display unmatch records\n",
    "    # show numbers\n",
    "    print(f'actual red companies: {df_compare.red_actual.sum()}')\n",
    "    print(f'actual black companies: {df_compare.black_actual.sum()}')\n",
    "    \n",
    "    print(f'predicted red companies: {df_compare.red_predicted.sum()}')\n",
    "    print(f'predicted black companies: {df_compare.black_predicted.sum()}')\n",
    "\n",
    "    # red: unmatch\n",
    "    if (~df_compare.red_match).sum() > 0:\n",
    "        print('red: unmatch, actual 1 predicted 0')\n",
    "        df_incorrect = df_compare[(df_compare['red_actual'] == 1) & (df_compare['red_predicted'] == 0)]\n",
    "        \n",
    "        print(df_incorrect.shape)\n",
    "        print(df_incorrect)\n",
    "\n",
    "        print('red: unmatch, actual 0 predicted 1(should definatily emply)')\n",
    "        print(df_compare[(df_compare['red_predicted'] == 1) & (df_compare['red_actual'] == 0)])\n",
    "    else:\n",
    "        print('Red: all match')\n",
    "\n",
    "    # black: unmatch\n",
    "    if (~df_compare.black_match).sum() > 0:\n",
    "        print('black: unmatch, actual 1 predicted 0')\n",
    "        print(df_compare[(df_compare['black_actual'] == 1) & (df_compare['black_predicted'] == 0)])\n",
    "\n",
    "        print('black: unmatch, actual 0 predicted 1(should definatily emply)')\n",
    "        print(df_compare[(df_compare['black_predicted'] == 1) & (df_compare['black_actual'] == 0)])\n",
    "    else:\n",
    "        print('Black: all match')\n",
    "    \n",
    "    # 3. SAVE unmatch records\n",
    "    print('save the unmatch records...')\n",
    "    with pd.ExcelWriter(os.path.join(processed_data_path, 'unmatch_credit_category_records_' + date_type + '.xlsx')) as writer:\n",
    "        # save red unmatch\n",
    "        df_compare[~df_compare.red_match].to_excel(writer, sheet_name='red_unmatch', freeze_panes=(1, 2))\n",
    "    \n",
    "        # save black unmatch\n",
    "        df_compare[~df_compare.black_match].to_excel(writer, sheet_name='black_unmatch', freeze_panes=(1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7262cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_panel_day_data(df):\n",
    "    \"\"\"\n",
    "    Expand the date in the day panel data to everyday.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        panel_data_day\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    df = df.reset_index().copy()\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Create a date range for each company\n",
    "    date_ranges = {}\n",
    "    for company in df['company_name'].unique():\n",
    "        company_data = df[df['company_name'] == company]\n",
    "        date_ranges[company] = pd.date_range(start=company_data['date'].min(),\n",
    "                                            end=company_data['date'].max(),\n",
    "                                            freq='D')\n",
    "\n",
    "    # Create an empty DataFrame to store the result\n",
    "    df_day = pd.DataFrame(columns=['company_name', 'date'])\n",
    "\n",
    "    # Fill in the rows for each company using their date range\n",
    "    for company, date_range in date_ranges.items():\n",
    "        filled_data = pd.DataFrame({'date': date_range, 'company_name': company})\n",
    "        df_day = pd.concat([df_day, filled_data])\n",
    "\n",
    "\n",
    "    df_day.date = pd.to_datetime(df_day.date)\n",
    "\n",
    "    df_day = df_day.merge(\n",
    "        df, how='left', on=['company_name', 'date']\n",
    "    ).set_index(\n",
    "        ['company_name', 'date']\n",
    "    ).groupby(level='company_name').ffill()\n",
    "    \n",
    "    return df_day\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728ad02",
   "metadata": {},
   "source": [
    "# easy access of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7101bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credit_panel_data(date_type='year', simple_version=True):\n",
    "    \"\"\"\n",
    "    Load and return panel data of year level or day level/full version or simple version.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_type : str, default 'year'\n",
    "        Can be 'day'\n",
    "    simple_version : bool, default True\n",
    "        Load simple version of panel data if True, otherwise load the original panel data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    # file path\n",
    "    filename = 'panel_data_' + date_type + ('_simple' if simple_version else '') + ('.xlsx' if date_type == 'year' else '.dta')\n",
    "    \n",
    "    path = os.path.join(processed_data_path, filename)\n",
    "    \n",
    "    if date_type == 'year':        \n",
    "        return pd.read_excel(path, index_col=[0, 1, 2])\n",
    "    else:\n",
    "        return pd.read_stata(path, index_col=[0, 1, 2]).rename(columns={\n",
    "            'blacklist_overload_transport_ill': 'blacklist_overload_transport_illegal'\n",
    "        }) # one column name was shortened when saving, need to recover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8966b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credit_meta_data():\n",
    "    \"\"\"\n",
    "    Load and return credit metadata.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    return pd.read_excel(os.path.join(processed_data_path, 'metadata.xlsx'), index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f5643dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mother_list():\n",
    "    \"\"\"\n",
    "    Load and return mother table with columns: ID, stock_code, company_name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    path = os.path.join(attri_path, 'mother_stock_codes.xlsx')\n",
    "    \n",
    "    return pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ada9d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stock_code(df, index_cols):\n",
    "    \"\"\"\n",
    "    Add stock code to dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    index_cols : list of str\n",
    "        The list of index columns of the df, e.g. ['ID', 'company_name', 'day'].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    df_stock = get_mother_list()\n",
    "    \n",
    "    return df.reset_index().merge(\n",
    "        df_stock[['company_name', 'stock_code']], \n",
    "        on='company_name', \n",
    "        how='left'\n",
    "    ).set_index(index_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0958c26",
   "metadata": {},
   "source": [
    "# Integrate performance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9a75d",
   "metadata": {},
   "source": [
    "## performance credit panel (mothers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_panel_data(df_meta=None):\n",
    "    \"\"\"\n",
    "    Load or create panel data of mother performance.\n",
    "    Only mothers have performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame, default None\n",
    "        meta data is used to add foundation_year.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    final_perf_path = os.path.join(performance_path, 'mother_performance.xlsx')\n",
    "    \n",
    "    if os.path.exists(final_perf_path):\n",
    "        return pd.read_excel(final_perf_path, index_col=[0, 1, 2])\n",
    "    \n",
    "    # not exist -> create performance data\n",
    "    print('create mother performance data...')\n",
    "    \n",
    "    # read original performance table\n",
    "    df_perf_m = pd.read_stata(os.path.join(original_path, 'mother_performance.dta'))\n",
    "    df_list_m = pd.read_excel(os.path.join(data_path, 'company_list', 'firm_list.xlsx'))\n",
    "    \n",
    "    # keep the columns needed    \n",
    "    columns_m = ['i', 't', 'name1', 'employee', \n",
    "                 'ownership', 'type_ownership', \n",
    "                 'IndustryCode', 'IndustryName', \n",
    "                 'province_con', 'LargestHolder', 'LargestHolderRate',\n",
    "                 'IApply', 'IApplyGrant', \n",
    "                 'subsidy', 'RD', \n",
    "                 'TotalAssets', 'TotalLiability', 'IntangibleAsset', \n",
    "                 'ProfitParent', 'NetProfit', 'OperatingEvenue',\n",
    "                 'OperatingCost', 'OperationProfit']\n",
    "\n",
    "    columns_rename_m = {\n",
    "        'i': 'stock_code',\n",
    "        't': 'year',\n",
    "        'IApply': 'patents_apply',\n",
    "        'IApplyGrant': 'patents_grant',\n",
    "        'ownership': 'organ_type',\n",
    "        'type_ownership': 'organ_type_code',\n",
    "        'name1': 'name'\n",
    "    }\n",
    "\n",
    "    df_perf = df_perf_m[columns_m].rename(columns=columns_rename_m)\n",
    "    \n",
    "    # only keep companies in credit dataset\n",
    "    # get name-code pairs in credit dataset\n",
    "    path = os.path.join(attri_path, 'mother_stock_codes.xlsx')\n",
    "    df_mothers = None\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        df_mothers = pd.read_excel(path)\n",
    "    else:    \n",
    "        # create and save df_mothers (stock_code, company_name)      \n",
    "        df_company = pd.read_excel(os.path.join(attri_path, 'company_ids.xlsx'))\n",
    "        \n",
    "        # mothers in credit dataset\n",
    "        mothers = list(set(df_list_m.company_name).intersection(set(df_company.company_name))) # 4258 unique company but 4346 stock codes\n",
    "        print(f'{len(mothers)} mothers')\n",
    "        \n",
    "        # add stock codes\n",
    "        df_mothers = df_list_m[df_list_m.company_name.isin(mothers)][['stock/foldername', 'company_name']].rename(\n",
    "            columns={'stock/foldername': 'stock_code'}) # DF: stock_code, company_name, use code to filter performance rows\n",
    "        \n",
    "        # add ID\n",
    "        df_mothers = assign_id_to_company(df_mothers)\n",
    "\n",
    "        df_mothers.to_excel(path, index=False)\n",
    "    \n",
    "    # handle rows: 1. only keep companyies in df_mothers, 2. year from 2007\n",
    "    perf_start_year = 2007\n",
    "    \n",
    "    df_perf = df_perf[(df_perf.stock_code.isin(df_mothers.stock_code)) & (df_perf.year >= perf_start_year)]\n",
    "    \n",
    "    # unify company name and add ID\n",
    "    df_perf = df_perf.merge(df_mothers, how='left', on='stock_code').drop(['name'], axis=1)\n",
    "    #df_perf = df_perf.merge(df_company, how='left', on='company_name')\n",
    "    \n",
    "    # add age\n",
    "    df_perf = df_perf.merge(\n",
    "        df_meta.reset_index()[['ID', 'foundation_year']],\n",
    "        on='ID',\n",
    "        how='left')\n",
    "    \n",
    "    print('check every firm has foundation_year: the number of NA in foundation_year')\n",
    "    print(df_perf.foundation_year.isnull().sum())\n",
    "    \n",
    "    df_perf['age'] = df_perf['year'] - df_perf['foundation_year'] + 1\n",
    "    \n",
    "    df_perf = df_perf[df_perf.age >= 0] # 4 companies have negative ages due to rename of firms\n",
    "    \n",
    "    df_perf = df_perf.drop(['foundation_year'], axis=1)\n",
    "    \n",
    "    # sort and set index\n",
    "    df_perf = df_perf.sort_values(['ID', 'year']).set_index(['ID', 'company_name', 'year'])\n",
    "    \n",
    "    # delete empty rows (only year stock_code name value, no other info)\n",
    "    df_perf = df_perf.replace(\n",
    "        r'^\\s*$', \n",
    "        np.nan, \n",
    "        regex=True\n",
    "    ).dropna(\n",
    "        subset=df_perf.columns[1:], \n",
    "        how='all') # columns[0] is stock_code which is always not NAN\n",
    "    \n",
    "    # handle NAN in organ_type and industry\n",
    "    def fillna_in_group(group):\n",
    "        group[['organ_type', 'organ_type_code',\n",
    "               'IndustryCode', 'IndustryName'\n",
    "              ]] = group[['organ_type', 'organ_type_code',\n",
    "                          'IndustryCode', 'IndustryName'\n",
    "                         ]].ffill().bfill()\n",
    "\n",
    "        return group\n",
    "\n",
    "    df_perf = df_perf.groupby(\n",
    "            level=[0,1]\n",
    "        ).apply(fillna_in_group)\n",
    "\n",
    "    # save and return\n",
    "    print('save performance data...')\n",
    "    df_perf.to_excel(final_perf_path, freeze_panes=(1, 3))\n",
    "    \n",
    "    return df_perf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24f297e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_credit_panel_data(df_perf=None, df_meta_year=None):\n",
    "    \"\"\"\n",
    "    Load or create performance_credit_panel_data for mothers. \n",
    "    The function combine mothers' credit panel data and performance panel data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_perf : DataFrame, default None\n",
    "        Performance panel data of mothers.\n",
    "    df_meta_year : DataFrame, default None\n",
    "        Credit panel data of mothers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    final_panel_path = os.path.join(performance_path, 'performance_credit_panel_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(final_panel_path):\n",
    "        return pd.read_excel(final_panel_path, index_col=[0, 1, 2])\n",
    "    \n",
    "    # load 2 panel data\n",
    "    if df_perf is None:\n",
    "        df_perf = get_performance_panel_data()\n",
    "    \n",
    "    if df_meta_year is None:\n",
    "        df_meta_year = get_credit_panel_data()\n",
    "    \n",
    "    # filter out mothers from df_meta_year\n",
    "    df_credit_mother = df_meta_year[\n",
    "        df_meta_year.index.get_level_values('ID').isin(\n",
    "            df_perf.index.get_level_values('ID').unique()\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # combine\n",
    "    df_combined_panel = df_credit_mother.join(df_perf, how='outer')\n",
    "    \n",
    "    print('save the combined panel data...')\n",
    "    df_combined_panel.to_excel(final_panel_path, freeze_panes=(1, 3))\n",
    "    \n",
    "    print('save the combined panel data but only from 2014...')\n",
    "    df_combined_panel[\n",
    "        df_combined_panel.index.get_level_values('year') >= (s_year - 1)\n",
    "    ].to_excel(\n",
    "        os.path.join(performance_path, 'performance_credit_panel_data_2014.xlsx'), \n",
    "        freeze_panes=(1, 3))\n",
    "    \n",
    "    return df_combined_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04470ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_credit_panel_data_2014():\n",
    "    \"\"\"\n",
    "    Load performance_credit_panel_data_2014 for mothers. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    final_panel_path = os.path.join(performance_path, 'performance_credit_panel_data_2014.xlsx')\n",
    "    \n",
    "    return pd.read_excel(final_panel_path, index_col=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d4646",
   "metadata": {},
   "source": [
    "## intergrate daguhters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ce6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mother_daughter_relation_panel_data(df_meta=None, df_meta_year=None):\n",
    "    \"\"\"\n",
    "    Load or create mother_daughter_relation_panel_data. \n",
    "    The panel data is a base table including mother and daughter correspondences over year, as well as daughters' credit info.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame, default None\n",
    "    df_meta_year : DataFrame, default None\n",
    "        Credit panel data(simple version).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    path = os.path.join(performance_path, 'mother_daughter_relation_panel_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print('load relation panel...')\n",
    "        return pd.read_excel(path, index_col=[0, 1, 2]).reset_index() # index just for decrease file size\n",
    "    \n",
    "    print('create relation panel...')\n",
    "    \n",
    "    # prepare tables\n",
    "    df_perf_d = pd.read_stata(os.path.join(original_path, 'daughter_performance.dta'))\n",
    "    \n",
    "    if df_meta is None:\n",
    "        df_meta = get_credit_meta_data()\n",
    "        \n",
    "    if df_meta_year is None:\n",
    "        df_meta_year = get_credit_panel_data()\n",
    "    \n",
    "    df_mothers = get_mother_list()\n",
    "    \n",
    "    # clean columns & handle stock code (6 digit str->int)\n",
    "    columns_needed = ['i', 't', 'RalatedParty', 'Relationship']\n",
    "\n",
    "    columns_rename = {\n",
    "        'i': 'mother_stock_code',\n",
    "        't': 'year',\n",
    "        'RalatedParty': 'daughter_name',\n",
    "        'Relationship': 'relationship'\n",
    "    }\n",
    "\n",
    "    df_relation_panel = df_perf_d[columns_needed].rename(columns=columns_rename).astype({'mother_stock_code': int})\n",
    "    \n",
    "    df_relation_panel.relationship = df_relation_panel.relationship.replace({\n",
    "        '上市公司的子公司': 'Subsidiary',\n",
    "        '上市公司的联营企业': 'Affiliate',\n",
    "        '上市公司的合营企业': 'Joint Venture'\n",
    "    })\n",
    "    \n",
    "    # remove doplicated 25102\n",
    "    df_relation_panel = df_relation_panel[~df_relation_panel.duplicated()]\n",
    "    \n",
    "    # add mother ID and name, filter out the mothers not in credit dataset\n",
    "    df_relation_panel = df_relation_panel.merge(\n",
    "        df_mothers, how='inner', left_on='mother_stock_code', right_on='stock_code'\n",
    "    ).rename(columns={\n",
    "        'company_name': 'mother_name',\n",
    "        'ID': 'mother_ID'\n",
    "    })\n",
    "    \n",
    "    # only observatons after 2014\n",
    "    df_relation_panel = df_relation_panel[df_relation_panel.year >= s_year]\n",
    "\n",
    "    # filter out daughters not in credit dataset\n",
    "    df_company = pd.read_excel(os.path.join(attri_path, 'company_ids.xlsx'))\n",
    "    \n",
    "    df_relation_panel = df_relation_panel.merge(\n",
    "        df_company, how='inner', left_on='daughter_name', right_on='company_name'\n",
    "    ).rename(columns={\n",
    "        'ID': 'daughter_ID'\n",
    "    })\n",
    "    \n",
    "    # add daughters' credit category and event numbers\n",
    "    df_relation_panel = df_relation_panel.merge(\n",
    "        df_meta_year.reset_index(), \n",
    "        how='left', \n",
    "        left_on=['daughter_name', 'year'], \n",
    "        right_on=['company_name', 'year'])\n",
    "    \n",
    "    # remove the observations whose year is earliar than their foundation year\n",
    "    df_year = df_meta.reset_index()[['ID', 'foundation_year']]\n",
    "    \n",
    "    df_relation_panel = df_relation_panel.merge(df_year, how='left', left_on='daughter_ID', right_on='ID') # add foundtion year\n",
    "    \n",
    "    df_relation_panel = df_relation_panel[df_relation_panel.year >= df_relation_panel.foundation_year]\n",
    "    \n",
    "    # keep columns needed\n",
    "    columns_order = ['mother_ID', 'mother_stock_code', 'mother_name', \n",
    "                     'year',\n",
    "                     'daughter_ID', 'daughter_name',\n",
    "                     'relationship',\n",
    "                     'green', 'red', 'black', \n",
    "                     'redlist', 'blacklist', 'watchlist', 'permit', 'penalty', 'commit']\n",
    "    \n",
    "    df_relation_panel = df_relation_panel[columns_order]\n",
    "    \n",
    "    # order\n",
    "    df_relation_panel = df_relation_panel.sort_values(['mother_ID', 'daughter_ID', 'year'])\n",
    "\n",
    "    # save and return\n",
    "    print('save panel...')\n",
    "    \n",
    "    df_relation_panel.set_index(['mother_ID', 'mother_name', 'mother_stock_code']).to_excel(path, freeze_panes=(1, 3))\n",
    "    \n",
    "    return df_relation_panel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cf5d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mother_daughter_stat_panel_data(df_relation_panel=None):\n",
    "    \"\"\"\n",
    "    Load or create relation_stat_panel_data. \n",
    "    The panel data is a aggregated panel table based on mother_daughter_relation_panel_data and including\n",
    "    mothers and their daughters stat info (number of different daughter type and number of their credit categoreis).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_relation_panel : DataFrame, default None\n",
    "        The panel data created by foundtion get_mother_daughter_relation_panel_data().\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    path = os.path.join(performance_path, 'relation_stat_panel_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print('load panel...')\n",
    "        return pd.read_excel(path, index_col=[0, 1, 2])\n",
    "    \n",
    "    print('create panel...')\n",
    "    \n",
    "    # dauther numbers\n",
    "    df_daughter_count = pd.crosstab(index=[df_relation_panel.mother_name, df_relation_panel.year], \n",
    "            columns=df_relation_panel.relationship)\n",
    "    \n",
    "    df_daughter_count = add_margins(df_daughter_count, 'total')\n",
    "    \n",
    "    # credit category numbers\n",
    "    df_credit_category_count = df_relation_panel.groupby(['mother_name', 'year'])[['green', 'red', 'black']].sum()\n",
    "    \n",
    "    # sum categories by company year relation\n",
    "    df_credit_relation_count = df_relation_panel.groupby(\n",
    "        ['mother_name', 'year', 'relationship']\n",
    "    )[['green', 'red', 'black']].sum(\n",
    "    ).reset_index(\n",
    "    ).pivot(index=['mother_name', 'year'],\n",
    "            columns='relationship',\n",
    "            values=['green', 'red', 'black']\n",
    "           )\n",
    "    \n",
    "    df_credit_relation_count.columns = ['-'.join(col) for col in df_credit_relation_count.columns.values] # flatten multiindex of columns\n",
    "    \n",
    "    # combine the 3 DF into one\n",
    "    df_relation_stat_panel = pd.concat([\n",
    "        df_daughter_count, \n",
    "        df_credit_category_count, \n",
    "        df_credit_relation_count\n",
    "    ], axis=1).reset_index().rename(columns={\n",
    "        'mother_name': 'company_name'\n",
    "    })\n",
    "    \n",
    "    # add ID and stock_code\n",
    "    df_relation_stat_panel = df_relation_stat_panel.pipe(\n",
    "        assign_id_to_company\n",
    "    ).set_index(['ID', 'company_name', 'year'])\n",
    "    \n",
    "    # save and return\n",
    "    df_relation_stat_panel.to_excel(path, freeze_panes=(1, 3))\n",
    "    \n",
    "    return df_relation_stat_panel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "245ecd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_credit_relationship_panel_data(df_credit_perf_panel=None, df_relation_stat_panel=None):\n",
    "    \"\"\"\n",
    "    Load or create performance_credit_relationship_panel_data. \n",
    "    The panel data inner merge mother_daughter_stat_panel_data to performance_credit_panel_data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_credit_perf_panel : DataFrame, default None\n",
    "        The panel contains data from 2007\n",
    "    df_relation_stat_panel : DataFrame, default None\n",
    "        The panel contains data from 2014\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\" \n",
    "    path = os.path.join(performance_path, 'credit_performance_relation_panel_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print('load panel...')\n",
    "        return pd.read_excel(path, index_col=[0, 1, 2])\n",
    "    \n",
    "    print('create panel...')\n",
    "    \n",
    "    # rename credit category in df_relation_stat_panel\n",
    "    df_relation_stat_panel = df_relation_stat_panel.rename(columns={\n",
    "        'green': 'green_daughters',\n",
    "        'red': 'red_daughters',\n",
    "        'black': 'black_daughters',\n",
    "        'total': 'total_daughters'\n",
    "    })\n",
    "    \n",
    "    # merge\n",
    "    df_final_panel = df_relation_stat_panel.join(df_combined_panel, how='outer')\n",
    "    \n",
    "    # refine year range\n",
    "    df_final_panel = df_final_panel[\n",
    "        (df_final_panel.index.get_level_values('year') >= s_year) &\n",
    "        (df_final_panel.index.get_level_values('year') < 2021)]\n",
    "    \n",
    "    # TODO: if the rows without relation or perf need to be deleted\n",
    "    \n",
    "    # save and return\n",
    "    df_final_panel.to_excel(path, freeze_panes=(1, 3))\n",
    "    \n",
    "    return df_final_panel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a06e8f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_meta = get_credit_meta_data()\\ndf_meta_year = get_credit_panel_data()\\n\\ndf_performance_panel = get_performance_panel_data(df_meta)\\n\\ndf_combined_panel = get_performance_credit_panel_data(df_performance_panel, df_meta_year) # for regression\\n\\ndf_relation_panel = get_mother_daughter_relation_panel_data(df_meta, df_meta_year)\\n\\ndf_relation_stat_panel = get_mother_daughter_stat_panel_data(df_relation_panel)\\n\\ndf_final_panel = get_performance_credit_relationship_panel_data(df_combined_panel, df_relation_stat_panel)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for generating performance data\n",
    "\n",
    "\"\"\"\n",
    "df_meta = get_credit_meta_data()\n",
    "df_meta_year = get_credit_panel_data()\n",
    "\n",
    "df_performance_panel = get_performance_panel_data(df_meta)\n",
    "\n",
    "df_combined_panel = get_performance_credit_panel_data(df_performance_panel, df_meta_year) # for regression\n",
    "\n",
    "df_relation_panel = get_mother_daughter_relation_panel_data(df_meta, df_meta_year)\n",
    "\n",
    "df_relation_stat_panel = get_mother_daughter_stat_panel_data(df_relation_panel)\n",
    "\n",
    "df_final_panel = get_performance_credit_relationship_panel_data(df_combined_panel, df_relation_stat_panel)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f434be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
