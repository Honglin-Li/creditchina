{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split_events\n",
    "\n",
    "Split and pre-processing metadata and event data.\n",
    "\n",
    "Contents:\n",
    "    INPUT\n",
    "    - data/original/mothers_original.xlsx\n",
    "    - data/original/daughters_original.xlsx\n",
    "\n",
    "    OUTPUT\n",
    "    - data/original/all_companies_original.xlsx\n",
    "    - data/clean_sub_events/metadata&xxx_sub_events.xlsx\n",
    "\n",
    "    PROCESS\n",
    "    - remove space around company_name\n",
    "    - merge mothers and daughters into one data source\n",
    "    - split 5 events to sub-events\n",
    "    - do some basis preprocessing for each split\n",
    "        - remove duplicates\n",
    "        - extract years\n",
    "        - fill missing values\n",
    "        - clean authority columns and fill out wrong ones with data sources\n",
    "        - unify np.nan, like ——， /， 无\n",
    "    - extract #sub_events to Metadata, add ID, re-order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c16650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for jupyter notebook\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parents[0].parents[0]))\n",
    "\n",
    "from src.data_transformation.utils import *\n",
    "from src.data_transformation.add_themes import add_penalty_themes, add_permit_themes, add_commitment_themes\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fce91d",
   "metadata": {},
   "source": [
    "# 1. Merge Daughters and Mothers\n",
    "- merge, drop the extra info columns in daughters(they are from those companies whose USCI start from none-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b90d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space_in_name(df, name = 'company_name'):\n",
    "    \"\"\"\n",
    "    Remove the space after company_name column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    name : str\n",
    "        The column name containing company names\n",
    "    \"\"\"\n",
    "    df[name] = df[name].str.strip()\n",
    "\n",
    "def combine_str_columns(row):\n",
    "    \"\"\"\n",
    "    Combine the text in all the cells in a row.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : Series\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    \"\"\"\n",
    "    return ''.join(row.dropna().astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7baeba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_companies():\n",
    "    \"\"\"\n",
    "    Merge mothers and daughters by meta and individual events, then save to local.\n",
    "    \n",
    "    Before merge, handle daughters metadata: remove space around company names, merge representative person column, \n",
    "    fill out empty address with approval authorities, merge firm types.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_meta : DataFrame\n",
    "        Merged meta DataFrame.\n",
    "    df_event1 : DataFrame\n",
    "        Merged event1 DataFrame.\n",
    "    df_event2 : DataFrame\n",
    "        Merged event2 DataFrame.\n",
    "    df_event3 : DataFrame\n",
    "        Merged event3 DataFrame.\n",
    "    df_event4 : DataFrame\n",
    "        Merged event4 DataFrame.\n",
    "    df_event5 : DataFrame\n",
    "        Merged event5 DataFrame.\n",
    "    \"\"\"\n",
    "    # if there is merged data, just return\n",
    "    merged_path = os.path.join(original_path, 'all_companies_original.xlsx')\n",
    "    \n",
    "    if os.path.exists(merged_path):\n",
    "        with pd.ExcelFile(merged_path) as xls:\n",
    "            df_meta = pd.read_excel(xls, sheet_name='metadata')\n",
    "            df_event1 = pd.read_excel(xls, sheet_name='event1')\n",
    "            df_event2 = pd.read_excel(xls, sheet_name='event2')\n",
    "            df_event3 = pd.read_excel(xls, sheet_name='event3')\n",
    "            df_event4 = pd.read_excel(xls, sheet_name='event4')\n",
    "            df_event5 = pd.read_excel(xls, sheet_name='event5')\n",
    "            \n",
    "        return df_meta, df_event1, df_event2, df_event3, df_event4, df_event5\n",
    "    \n",
    "    # load mothers and daughters\n",
    "    with pd.ExcelFile(os.path.join(original_path, 'mothers_original.xlsx')) as xls:\n",
    "        df_meta_m = pd.read_excel(xls, sheet_name='metadata')\n",
    "        df_event1_m = pd.read_excel(xls, sheet_name='event1')\n",
    "        df_event2_m = pd.read_excel(xls, sheet_name='event2')\n",
    "        df_event3_m = pd.read_excel(xls, sheet_name='event3')\n",
    "        df_event4_m = pd.read_excel(xls, sheet_name='event4')\n",
    "        df_event5_m = pd.read_excel(xls, sheet_name='event5')\n",
    "\n",
    "    with pd.ExcelFile(os.path.join(original_path, 'daughters_original.xlsx')) as xls:\n",
    "        df_meta_d = pd.read_excel(xls, sheet_name='metadata')\n",
    "        df_event1_d = pd.read_excel(xls, sheet_name='event1')\n",
    "        df_event2_d = pd.read_excel(xls, sheet_name='event2')\n",
    "        df_event3_d = pd.read_excel(xls, sheet_name='event3')\n",
    "        df_event4_d = pd.read_excel(xls, sheet_name='event4')\n",
    "        df_event5_d = pd.read_excel(xls, sheet_name='event5')\n",
    "    \n",
    "    # handle daughters meta\n",
    "    # address\n",
    "    \n",
    "    df_meta_d['住所 Address'] = df_meta_d[['住所 Address', '审批机关 Approving authority']].apply(combine_str_columns, axis=1)\n",
    "    #df_meta_d['住所 Address'].fillna('').astype(str) + \\\n",
    "        #df_meta_d['审批机关 Approving authority'].fillna('').astype(str)\n",
    "    \n",
    "    # representative\n",
    "    df_meta_d['法定代表/负责人/执行事务合伙人 Legal representative/Person in charge/Executive business partner'] = \\\n",
    "        df_meta_d[['法定代表/负责人/执行事务合伙人 Legal representative/Person in charge/Executive business partner', \n",
    "            '法定代表人姓名 Name of Legal representative',\n",
    "            '法定代表人 Legal representative'\n",
    "           ]].apply(combine_str_columns, axis=1)\n",
    "       # df_meta_d['法定代表/负责人/执行事务合伙人 Legal representative/Person in charge/Executive business partner'].fillna('').astype(str) + \\\n",
    "       # df_meta_d['法定代表人姓名 Name of Legal representative'].fillna('').astype(str) + \\\n",
    "       # df_meta_d['法定代表人 Legal representative'].fillna('').astype(str)\n",
    "    \n",
    "    # firm type\n",
    "    df_meta_d['企业类型 Corporate type'] = df_meta_d[['企业类型 Corporate type', \n",
    "                                              '组织类型 Organization type'\n",
    "                                          ]].apply(combine_str_columns, axis=1)\n",
    "    #df_meta_d['企业类型 Corporate type'].fillna('').astype(str) + \\\n",
    "       # df_meta_d['组织类型 Organization type'].fillna('').astype(str)\n",
    "    \n",
    "    # drop the extra columns in metadata of daughters, then merge\n",
    "    df_meta = pd.concat([df_meta_m, \n",
    "                        df_meta_d[['统一社会信用代码 Unified Social Credit Identifier, USCI',\n",
    "                                   '机构名称 Institution name', \n",
    "                                   '企业类型 Corporate type', \n",
    "                                   '住所 Address',\n",
    "                                   '法定代表/负责人/执行事务合伙人 Legal representative/Person in charge/Executive business partner',\n",
    "                                   '成立日期 Date of Foundation', \n",
    "                                   '行政管理 Administrative management',\n",
    "                                   '严重失信主体名单 Severe untrustworthy entity list', \n",
    "                                   '信用承诺 Credit commitment',\n",
    "                                   '司法判决 Judicial decision', \n",
    "                                   '诚实守信 Honesty and trustworthy',\n",
    "                                   '经营异常 Abnormal operation', \n",
    "                                   '信用评价 Credit assessment', \n",
    "                                   '其他信息 Other information', \n",
    "                                   'black',\n",
    "                                   'green', \n",
    "                                   'red', \n",
    "                                   'grey',\n",
    "                                   'event_count', \n",
    "                                   'n_pages',\n",
    "                                   'M_D']]])\n",
    "    df_meta.grey = df_meta.grey.fillna(0)\n",
    "    \n",
    "    # remove space in company_name\n",
    "    remove_space_in_name(df_meta, '机构名称 Institution name')\n",
    "\n",
    "    df_event1 = pd.concat([df_event1_m, df_event1_d])\n",
    "    remove_space_in_name(df_event1)\n",
    "    \n",
    "    df_event2 = pd.concat([df_event2_m, df_event2_d])\n",
    "    remove_space_in_name(df_event2)\n",
    "    \n",
    "    df_event3 = pd.concat([df_event3_m, df_event3_d])\n",
    "    remove_space_in_name(df_event3)\n",
    "    \n",
    "    df_event4 = pd.concat([df_event4_m, df_event4_d])\n",
    "    remove_space_in_name(df_event4)\n",
    "    \n",
    "    df_event5 = pd.concat([df_event5_m, df_event5_d])\n",
    "    remove_space_in_name(df_event5)\n",
    "    \n",
    "    # save the merged version\n",
    "    with pd.ExcelWriter(merged_path) as xls:\n",
    "        df_meta.to_excel(xls, sheet_name='metadata', index=False, freeze_panes=(1, 2))\n",
    "        df_event1.to_excel(xls, sheet_name='event1', index=False, freeze_panes=(1, 2))\n",
    "        df_event2.to_excel(xls, sheet_name='event2', index=False, freeze_panes=(1, 2))\n",
    "        df_event3.to_excel(xls, sheet_name='event3', index=False, freeze_panes=(1, 2))\n",
    "        df_event4.to_excel(xls, sheet_name='event4', index=False, freeze_panes=(1, 2))\n",
    "        df_event5.to_excel(xls, sheet_name='event5', index=False, freeze_panes=(1, 2))\n",
    "    \n",
    "    return df_meta, df_event1, df_event2, df_event3, df_event4, df_event5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5e7fc",
   "metadata": {},
   "source": [
    "# 2. USCI\n",
    "- USCI: extract organizational type. because few companies not start from 9(business)\n",
    "- region-USCI (Some codes do not match the 2020 area codes）\n",
    "\n",
    "## 2.1 extract organizational type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952e6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_organ_types(df_meta):\n",
    "    \"\"\"\n",
    "    Add organ_type column from USCI to df_meta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The df_meta with a new column: organ_type.\n",
    "    \"\"\"\n",
    "    # load organizational type\n",
    "    region_path = os.path.join(attri_path, 'regions.xlsx')\n",
    "    df_organ_type = pd.read_excel(region_path, sheet_name='USCI', dtype={'code12': str})[['code12', 'type', 'type_en']]\n",
    "\n",
    "    # look into organizational type\n",
    "    organ_type_stat = df_meta[usci].str[:2].value_counts().to_frame().reset_index().rename(\n",
    "            columns={'index': 'code12',\n",
    "                    usci: 'count'})\n",
    "    organ_type_stat.merge(df_organ_type, how='left')\n",
    "\n",
    "    # from the result above, we found 2 codes without info, so add code12=22 manually\n",
    "    df_organ_type = pd.concat([df_organ_type, pd.DataFrame([\n",
    "        {'code12': '22', 'type': '外交', 'type_en': 'Diplomatic'},\n",
    "        {'code12': '——', 'type': '没有USCI', 'type_en': 'no USCI'}\n",
    "    ])], ignore_index=True)\n",
    "\n",
    "    display(df_organ_type.tail())\n",
    "\n",
    "    # add organizational type to metadata\n",
    "    organ_type_dict = dict(zip(df_organ_type['code12'], df_organ_type['type_en']))\n",
    "    df_meta['organ_type'] = df_meta[usci].str[:2].replace(organ_type_dict)\n",
    "    return df_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb90eb",
   "metadata": {},
   "source": [
    "## 2.2 extract Region: level, region, province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af02fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regions(df_meta):\n",
    "    \"\"\"\n",
    "    Extract and add level, region, province columns to df_meta from USCI.\n",
    "    \n",
    "    If there is no corresponding region info in the USCI, extract region info from address column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_meta : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The df_meta with 3 region columns.\n",
    "    \"\"\"\n",
    "    def combine_data_sources(df_meta, df_pca):\n",
    "        na_id = df_meta[df_meta.province.isnull()].index\n",
    "        df_meta.loc[na_id, 'level'] = df_pca.loc[na_id, 'level']\n",
    "        df_meta.loc[na_id, 'province'] = df_pca.loc[na_id, 'province']\n",
    "        df_meta.loc[na_id, 'region'] = df_pca.loc[na_id, 'region']\n",
    "        \n",
    "    def change_individual_region(df_meta, wrong_province_value, l, p, r):\n",
    "        na_id = df_meta[df_meta.province == wrong_province_value].index\n",
    "        df_meta.loc[na_id, 'level'] = l\n",
    "        df_meta.loc[na_id, 'province'] = p\n",
    "        df_meta.loc[na_id, 'region'] = r\n",
    "        \n",
    "    # load region: code, level, region, province\n",
    "    region_path = os.path.join(attri_path, 'regions.xlsx')\n",
    "    df_region = pd.read_excel(region_path, dtype={'code': str}, sheet_name='CODE6')\n",
    "    \n",
    "    # DS1: add region from usci\n",
    "    df_meta['code'] = df_meta[usci].str[2:8]\n",
    "    df_meta = df_meta.merge(df_region, how='left', on='code')\n",
    "    \n",
    "    # check how many companies have no corresponding region\n",
    "    print(f'{df_meta.level.isnull().sum()} companies have no corresponding region info from their USCI, need to extract from address')\n",
    "    \n",
    "    # DS2: extract region info from address\n",
    "    df_pca = extract_regions(df_meta['住所 Address']) \n",
    "    df_pca['region'] = df_pca[['province', 'city', 'area']].apply(combine_str_columns, axis=1) # province| city| area| level | address | region\n",
    "    \n",
    "    # combine the 2 data source\n",
    "    combine_data_sources(df_meta, df_pca)\n",
    "    \n",
    "    # manually change 2 errors in region\n",
    "    change_individual_region(df_meta, '洪泽经济开发区328省', 'area', '江苏省', '江苏省淮安市洪泽区')\n",
    "    change_individual_region(df_meta, '京山市雁门口镇(107省', 'area', '湖北省', '湖北省荆门市京山市雁门口镇')\n",
    "    \n",
    "    # DS3: for those without province, use company_name search for pca\n",
    "    df_pca = extract_regions(df_meta[company_name]) \n",
    "    df_pca['region'] = df_pca[['province', 'city', 'area']].apply(combine_str_columns, axis=1)\n",
    "    \n",
    "    # combine the 2 data source\n",
    "    combine_data_sources(df_meta, df_pca)\n",
    "    \n",
    "    \n",
    "    # drop code6\n",
    "    df_meta.drop(['code'], axis=1, inplace=True)\n",
    "\n",
    "    return df_meta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26015c2e",
   "metadata": {},
   "source": [
    "# 3. Split events to sub-events\n",
    "- split\n",
    "- preprocessing: fill missing values, drop duplicates, remove useless columns, merge events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5668f",
   "metadata": {},
   "source": [
    "### common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcb3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal(df_event, bool_list):\n",
    "    \"\"\"\n",
    "    Check if the sub-events are splited correctly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event : DataFrame\n",
    "    bool_list : list of bool Series\n",
    "        The item is a bool Series which mark each row is belong to a sub-event or not.\n",
    "    \"\"\"\n",
    "    sub_sum = sum([l.sum() for l in bool_list])\n",
    "    total = df_event.shape[0]\n",
    "    print(f'the number of sub events:{sub_sum}')\n",
    "    print(f'total number: {total}')\n",
    "    \n",
    "    equal = total - sub_sum\n",
    "    if equal == 0:\n",
    "        print('Correct')\n",
    "    else:\n",
    "        print('Incorrect, please check the numbers of sub-events')\n",
    "        for l in bool_list:\n",
    "            print(l.sum())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389e62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df_events):\n",
    "    \"\"\"\n",
    "    Check duplicates of events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_events : list of DataFrame\n",
    "        Each DataFrame is for a sub-event.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of int\n",
    "        Each item is the number of duplicates of the sub-event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    print('duplicates:')\n",
    "    \n",
    "    for df in df_events:\n",
    "        result = df.duplicated().sum()\n",
    "        print(result)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454cfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(df, date_column_name, prefix, fill_value=np.nan):\n",
    "    \"\"\"\n",
    "    Extract year from a date column & rename the date column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        A DataFrame containing a date column.\n",
    "    date_column_name : str\n",
    "        The name of the data column.\n",
    "    prefix : str\n",
    "        The prefix of date and year columns, like \"start_\" or \"end_\".\n",
    "    fill_value : str or int, default np.nan\n",
    "        If the date is nan, fill NA by fill_value.\n",
    "    \"\"\"\n",
    "    # if date is not a date, fill a value\n",
    "    df[prefix + 'year'] = df[date_column_name].str[:4].replace({'— —': fill_value})\n",
    "    \n",
    "    return df.rename(columns={\n",
    "        date_column_name : prefix + 'date'\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf69476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(text):\n",
    "    \"\"\"\n",
    "    Return if a text contains numbers(>=2).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    return bool(re.search(r'\\d{2,}', str(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1144442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_start_year(df_event, source_column):\n",
    "    \"\"\"\n",
    "    Extract start date and year columns from content column for those events without start dates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event : DataFrame\n",
    "    source_column : str\n",
    "        The column name which contains dates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A DataFrame with start year and missing flag columns.\n",
    "    \"\"\"\n",
    "    date_pattern = r'(\\d{4})年(\\d{1,2})月(\\d{1,2})?日?'\n",
    "    \n",
    "    # func to set start date\n",
    "    def get_date(x): # x: list of tuples, may like (2019, 1, 31) or (2019, 1, )\n",
    "        if len(x):\n",
    "            x = x[-1] # get the last date tuple\n",
    "            \n",
    "            # add last day\n",
    "            if not x[2]:\n",
    "                last_day_of_month = calendar.monthrange(int(x[0]), int(x[1]))[1]\n",
    "                x = list(x)\n",
    "                x[2] = str(last_day_of_month)\n",
    "                \n",
    "            return '-'.join(x)\n",
    "        \n",
    "        return np.nan\n",
    "        \n",
    "    \n",
    "    df_event['start_date'] = df_event[source_column].str.findall(date_pattern).map(get_date)\n",
    "    \n",
    "    # fill NA with MAX year\n",
    "    if df_event.start_date.isnull().sum():\n",
    "        # get max date\n",
    "        max_date = pd.to_datetime(df_event.start_date).max().strftime('%Y-%m-%d')\n",
    "        df_event['flag'] = np.nan\n",
    "\n",
    "        df_event.loc[df_event.start_date.isnull(), 'flag'] = 'Missing date'\n",
    "        df_event.start_date = df_event.start_date.fillna(max_date)\n",
    "    \n",
    "    # add year\n",
    "    df_event['start_year'] = df_event.start_date.str[:4]\n",
    "    #extract_year(df_event, 'start_date', 'start_')\n",
    "    \n",
    "    return df_event\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902440c9",
   "metadata": {},
   "source": [
    "## 2.1 event1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3293be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_event1(df_event1):\n",
    "    \"\"\"\n",
    "    Split event1 into permit and penalty.\n",
    "    \n",
    "    The processing contains extract year, fill incorrect authorites by data sources, fill na, drop dupliacates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event1 : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_event_permit : DataFrame\n",
    "    df_event_penalty : DataFrame\n",
    "    \"\"\"\n",
    "    # filter condition to get sub-events\n",
    "    bool_event_permit_old = df_event1['审核类型 Audit Type'].notna()\n",
    "    bool_event_permit_new = df_event1['许可类别 Type of Permission'].notna()\n",
    "    bool_event_penalty = df_event1['处罚机关 Penalty Enforcement Authority'].notna()\n",
    "    \n",
    "    # check the total amount\n",
    "    check_equal(df_event1, [bool_event_permit_old, bool_event_permit_new, bool_event_penalty])\n",
    "    # difference is 37. the 37 entries are from non-business companies, so can be ignored\n",
    "    \n",
    "    # get the df for all the sub-events\n",
    "    df_event_permit_old = df_event1.loc[bool_event_permit_old, ['统一社会信用代码 Unified Social Credit Identifier', \n",
    "                                       'company_name',\n",
    "                                       '行政许可决定文书号 Administrative Permission Decision Document Code', \n",
    "                                       '许可决定日期 Permission decision date', \n",
    "                                       '许可内容 Permission Content', \n",
    "                                       '许可机关 Permission Authority', \n",
    "                                       '许可截止日期 Permission deadline date', \n",
    "                                       '审核类型 Audit Type']].reset_index(drop=True)\n",
    "\n",
    "    df_event_permit_new = df_event1.loc[bool_event_permit_new, [\n",
    "        '统一社会信用代码 Unified Social Credit Identifier', \n",
    "        'company_name',\n",
    "        '行政许可决定文书号 Administrative Permission Decision Document Code',\n",
    "        '行政许可决定文书名称 Name of Administrative Permission Decision',\n",
    "        '许可证书名称 Name of Permission Certificate', \n",
    "        '许可类别 Type of Permission',\n",
    "        '许可编号 Number of Permission', \n",
    "        '许可决定日期 Permission decision date',\n",
    "        '有效期自 Permission  Decision Valid From', \n",
    "        '有效期至 Valid Until',\n",
    "        '许可内容 Permission Content', \n",
    "        '许可机关 Permission Authority',\n",
    "        '许可机关统一社会信用代码 Permission Authority USCI', \n",
    "        '数据来源单位 Data Sources Unit',\n",
    "        '数据来源单位统一社会信用代码 Data Sources Unit USCI']].reset_index(drop=True)\n",
    "\n",
    "    df_event_penalty = df_event1.loc[bool_event_penalty, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '行政处罚决定书文号 Administrative Penalty Decision Document Number',\n",
    "            '处罚类别 Penalty Type', \n",
    "            '处罚决定日期 Penalty Date', \n",
    "            '处罚内容 Penalty Content',\n",
    "            '罚款金额(万元) Fine Amount, 10k Yuan',\n",
    "            '没收违法所得、没收非法财物的金额（万元）Confiscation of Illegal Gains, 10k Yuan',\n",
    "            '暂扣或吊销证照名称及编号 Suspension or Revocation of License Name and Number',\n",
    "            '违法行为类型 Type of Illegal Behavior', \n",
    "            '违法事实 Illegal Facts',\n",
    "            '处罚依据 Penalty Basis', \n",
    "            '处罚机关 Penalty Enforcement Authority',\n",
    "            '处罚机关统一社会信用代码 Penalty Enforcement Authority USCI', \n",
    "            '数据来源 Data sources',\n",
    "            '数据来源单位统一社会信用代码 Data Sources Unit USCI']].reset_index(drop=True)\n",
    "    \n",
    "    # check duplicates\n",
    "    results = check_duplicates([df_event_permit_old, df_event_permit_new, df_event_penalty])\n",
    "    \n",
    "    # preprocessing: drop duplicates\n",
    "    if results[0] > 0:\n",
    "        df_event_permit_old.drop_duplicates(inplace=True)\n",
    "    if results[1] > 0:\n",
    "        df_event_permit_new.drop_duplicates(inplace=True)\n",
    "    if results[2] > 0:\n",
    "        df_event_penalty.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # preprocessing: permit old\n",
    "    df_event_permit_old = df_event_permit_old.pipe(\n",
    "        extract_year, \n",
    "        '许可决定日期 Permission decision date', \n",
    "        'start_'\n",
    "    ).pipe(extract_year, \n",
    "           '许可截止日期 Permission deadline date', \n",
    "           'end_', \n",
    "           '2099')\n",
    "    \n",
    "    # permit new\n",
    "    # extract year\n",
    "    df_event_permit_new = df_event_permit_new.pipe(\n",
    "        extract_year, \n",
    "        '许可决定日期 Permission decision date', \n",
    "        'start_'\n",
    "    ).pipe(extract_year, \n",
    "           '有效期至 Valid Until', \n",
    "           'end_', \n",
    "           '2099')\n",
    "    \n",
    "    # fill out missing permit authorities by data source\n",
    "    # if authority has numbers, means a wrong value\n",
    "    # authorities is mess, like 浙江省行政权力运行系统-2, 2022-09-31, 东环函〔2022〕13号, 39f58a70-a952-4ab2-bf3f-f1aea2655584, 闽侯县城乡规划局(2), 汕头市自然资源局(根据省政府令第270号,受省林业局委托), 信阳市-信阳市产业集聚区-信阳市工业城工五路10号\n",
    "\n",
    "    tmp = df_event_permit_new['许可机关 Permission Authority'].apply(has_numbers)\n",
    "    wrong_auth_index = tmp[tmp].index\n",
    "    print('those authorities will be filled out by data source')\n",
    "    display(df_event_permit_new.loc[wrong_auth_index, ['许可机关 Permission Authority', '数据来源单位 Data Sources Unit']].head())\n",
    "    \n",
    "    # fill wrong auth\n",
    "    df_event_permit_new.loc[wrong_auth_index, '许可机关 Permission Authority'] = df_event_permit_new.loc[wrong_auth_index, '数据来源单位 Data Sources Unit']\n",
    "    df_event_permit_new.loc[wrong_auth_index, '许可机关统一社会信用代码 Permission Authority USCI'] = df_event_permit_new.loc[wrong_auth_index, '数据来源单位统一社会信用代码 Data Sources Unit USCI']\n",
    "    \n",
    "    # delete useless columns\n",
    "    df_event_permit_new.drop(['有效期自 Permission  Decision Valid From', '许可编号 Number of Permission', '数据来源单位 Data Sources Unit', '数据来源单位统一社会信用代码 Data Sources Unit USCI'], axis=1, inplace=True)\n",
    "    \n",
    "    # combine permit old and new version\n",
    "    df_event_permit = pd.concat([df_event_permit_new, df_event_permit_old.rename(\n",
    "        columns={'审核类型 Audit Type': '许可类别 Type of Permission',\n",
    "                '许可截止日期 Permission deadline date': '有效期至 Valid Until'})])\n",
    "    df_event_permit['permit_type'] = df_event_permit['许可类别 Type of Permission'].str[:2] # remove the text after type\"others\"\n",
    "    \n",
    "    # PENALTY\n",
    "    # year\n",
    "    df_event_penalty = df_event_penalty.pipe(\n",
    "        extract_year, \n",
    "        '处罚决定日期 Penalty Date', \n",
    "        'start_'\n",
    "    )\n",
    "    \n",
    "    # penalty auth\n",
    "    # some wrong values with date, remove date\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'(\\s处罚日期:)?\\d{4}年\\d{1,2}月\\d{1,2}日', '', text)\n",
    "\n",
    "\n",
    "    df_event_penalty['处罚机关 Penalty Enforcement Authority'] = df_event_penalty['处罚机关 Penalty Enforcement Authority'].apply(\n",
    "        clean_text)\n",
    "\n",
    "    # numbers\n",
    "    tmp = df_event_penalty['处罚机关 Penalty Enforcement Authority'].apply(has_numbers)\n",
    "    wrong_auth_index = tmp[tmp].index\n",
    "    print('those authorities will be filled out by data sources')\n",
    "    display(df_event_penalty.loc[wrong_auth_index, ['处罚机关 Penalty Enforcement Authority', '数据来源 Data sources']])\n",
    "    \n",
    "    df_event_penalty.loc[wrong_auth_index, '处罚机关 Penalty Enforcement Authority'] = df_event_penalty.loc[wrong_auth_index, '数据来源 Data sources']\n",
    "    df_event_penalty.loc[wrong_auth_index, '处罚机关统一社会信用代码 Penalty Enforcement Authority USCI'] = df_event_penalty.loc[wrong_auth_index, '数据来源单位统一社会信用代码 Data Sources Unit USCI']\n",
    "    \n",
    "    # change money missing values to 0\n",
    "    df_event_penalty['罚款金额(万元) Fine Amount, 10k Yuan'] = df_event_penalty['罚款金额(万元) Fine Amount, 10k Yuan'].replace({\n",
    "        '— —': 0\n",
    "    })\n",
    "    df_event_penalty['没收违法所得、没收非法财物的金额（万元）Confiscation of Illegal Gains, 10k Yuan'] = df_event_penalty['没收违法所得、没收非法财物的金额（万元）Confiscation of Illegal Gains, 10k Yuan'].replace({\n",
    "        '— —': 0\n",
    "    })\n",
    "    \n",
    "    # remove values with null meaning to null\n",
    "    df_event_penalty['暂扣或吊销证照名称及编号 Suspension or Revocation of License Name and Number'] = df_event_penalty[\n",
    "        '暂扣或吊销证照名称及编号 Suspension or Revocation of License Name and Number'].replace(\n",
    "        {'— —': np.nan,\n",
    "        '无': np.nan,\n",
    "        '/': np.nan})\n",
    "    \n",
    "    df_event_penalty.drop(['数据来源 Data sources', '数据来源单位统一社会信用代码 Data Sources Unit USCI'], axis=1, inplace=True)\n",
    "    \n",
    "    # Add authority theme and save permit and penalty\n",
    "    df_event_permit = add_permit_themes(df_event_permit)\n",
    "    #df_event_penalty = add_penalty_themes(df_event_penalty)\n",
    "    \n",
    "    # save the 4 sub-events\n",
    "    print('saving(take time)...')\n",
    "    df_event_permit_old.to_excel(os.path.join(sub_event_path, '13_event_permit_old.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_permit_new.to_excel(os.path.join(sub_event_path, '14_event_permit_new.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_permit.to_excel(os.path.join(sub_event_path, '11_event_permit.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_penalty.to_excel(os.path.join(sub_event_path, '12_event_penalty.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "          \n",
    "    return df_event_permit, df_event_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852b260",
   "metadata": {},
   "source": [
    "## 2.2 Split event2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac664a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_event2(df_event2):\n",
    "    \"\"\"\n",
    "    Split event2 into 4 types of redlists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event2 : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_event_custom : DataFrame\n",
    "    df_event_a_taxpayer : DataFrame\n",
    "    df_event_highway : DataFrame\n",
    "    df_event_transportation : DataFrame\n",
    "    \"\"\"\n",
    "    # row condition\n",
    "    bool_event_custom = df_event2['海关注册编码 Customs Record Number'].notna()\n",
    "    bool_event_a_taxpayer = df_event2['纳税人名称 Taxpayer name'].notna()\n",
    "    bool_event_highway = df_event2['企业资质 Firm Qualification'].notna()\n",
    "    bool_event_transportation = df_event2['文件依据 Document basis'].notna()\n",
    "    \n",
    "    check_equal(df_event2, [bool_event_custom, bool_event_a_taxpayer, bool_event_highway, bool_event_transportation])\n",
    "    \n",
    "    # split\n",
    "    df_event_custom = df_event2.loc[bool_event_custom, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '组织机构代码 Organizational institution identifier',\n",
    "            '海关注册编码 Customs Record Number', \n",
    "            '首次注册日期 First registration date',\n",
    "            '等级认定时间 Accreditation time',\n",
    "            '数据来源 Data sources']].reset_index(drop=True)\n",
    "\n",
    "    df_event_a_taxpayer = df_event2.loc[bool_event_a_taxpayer, [\n",
    "        '统一社会信用代码 Unified Social Credit Identifier', \n",
    "        'company_name',\n",
    "        '评价年度 Evaluation year',\n",
    "        '数据来源 Data sources']].reset_index(drop=True) # taxpayer and id repeat\n",
    "\n",
    "    df_event_highway = df_event2.loc[bool_event_highway, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '注册地址 Registration address', \n",
    "            '企业资质 Firm Qualification', \n",
    "            '年度 Year',\n",
    "            '备注 Comment',\n",
    "            '数据来源 Data sources']].reset_index(drop=True)\n",
    "\n",
    "    df_event_transportation = df_event2.loc[bool_event_transportation, [\n",
    "        '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '文件依据 Document basis',\n",
    "            '数据来源 Data sources']].reset_index(drop=True)\n",
    "    \n",
    "    # duplicates\n",
    "    results = check_duplicates([df_event_custom, df_event_a_taxpayer, df_event_highway, df_event_transportation])\n",
    "    \n",
    "    df_event_a_taxpayer.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # custom\n",
    "    df_event_custom = df_event_custom.pipe(\n",
    "        extract_year, \n",
    "        '首次注册日期 First registration date', \n",
    "        'register_'\n",
    "    ).pipe(\n",
    "    extract_year,\n",
    "    '等级认定时间 Accreditation time', \n",
    "    'level_')\n",
    "    \n",
    "    # save\n",
    "    print('saving...')\n",
    "    df_event_custom.to_excel(os.path.join(sub_event_path, '21_event_custom.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_a_taxpayer.to_excel(os.path.join(sub_event_path, '22_event_a_taxpayer.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_highway.to_excel(os.path.join(sub_event_path, '23_event_highway.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_transportation.to_excel(os.path.join(sub_event_path, '24_event_transportation.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    \n",
    "    return df_event_custom, df_event_a_taxpayer, df_event_highway, df_event_transportation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7b1d5",
   "metadata": {},
   "source": [
    "## 2.3 split event3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b35127af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_event3(df_event3):\n",
    "    \"\"\"\n",
    "    Split event3 into 5 types of blacklists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event3 : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_event_dishonest_person : DataFrame\n",
    "    df_event_safety_production : DataFrame\n",
    "    df_event_tax_blacklist : DataFrame\n",
    "    df_event_gov_procure_illegal : DataFrame\n",
    "    df_event_overload_transport_illegal : DataFrame\n",
    "    \"\"\"\n",
    "    bool_event_dishonest_person = df_event3['省份  Province'].notna()\n",
    "    bool_event_safety_production = df_event3['纳入理由 Reason for inclusion'].notna()\n",
    "    bool_event_tax_blacklist = df_event3['纳税人识别号 Taxpayer identification number'].notna()\n",
    "    bool_event_gov_procure_illegal = df_event3['处罚截止日期 Penalty deadline'].notna()\n",
    "    bool_event_overload_transport_illegal = df_event3['入库时间 Registration time'].notna()\n",
    "    check_equal(df_event3, [bool_event_dishonest_person, bool_event_safety_production, bool_event_tax_blacklist, bool_event_gov_procure_illegal, bool_event_overload_transport_illegal])\n",
    "    \n",
    "    # split\n",
    "    df_event_dishonest_person = df_event3.loc[bool_event_dishonest_person, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '执行法院 Execution Court',\n",
    "            '省份  Province', \n",
    "            '执行依据文号 Execution Base Number', \n",
    "            '立案时间 Date of Filing',\n",
    "            '案号 Case Number', \n",
    "            '做出执行依据单位 Execution Base Unit',\n",
    "            '生效法律文书确定的义务 Obligations Determined by Effective Legal Documents',\n",
    "            '被执行人的履行情况 Implementation Performance of Person of Execution',\n",
    "            '失信被执行人行为具体情形 Person of Execution Untrustworthy Behavior Details',\n",
    "            '发布时间 Date of Issue', \n",
    "            '已履行部分 Part of Accomplishment',\n",
    "            '未履行部分 Part of Non-accomplishment', \n",
    "            '数据来源 Data sources'\n",
    "    ]].reset_index(drop=True)\n",
    "    \n",
    "    # change province(short version) in system to province in standard version\n",
    "    df_province_trans = pd.read_excel(os.path.join(attri_path, 'regions.xlsx'), sheet_name='province_short', index_col=0)\n",
    "    \n",
    "    df_event_dishonest_person['省份  Province'] = df_event_dishonest_person['省份  Province'].map(\n",
    "        df_province_trans.province.to_dict()\n",
    "    ) \n",
    "\n",
    "    df_event_safety_production = df_event3.loc[bool_event_safety_production, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '主要负责人 Principal', \n",
    "            '注册地址 Registration address',\n",
    "            '失信行为简况 Default behavior profile',\n",
    "            '信息报送机关 Information reporting authority', \n",
    "            '纳入理由 Reason for inclusion',\n",
    "            '数据来源 Data sources'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    df_event_tax_blacklist = df_event3.loc[bool_event_tax_blacklist, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '纳税人名称 Taxpayer name',\n",
    "            '法定代表人或负责人姓名 Name of legal representative or person in charge',\n",
    "            '纳税人识别号 Taxpayer identification number', \n",
    "            '案件上报期 Case reporting period',\n",
    "            '负有直接责任的财务负责人姓名 Name of the financial person directly responsible',\n",
    "            '负有直接责任的中介机构信息及其从业人员信息 Information of intermediaries with direct responsibility and information of their practitioners',\n",
    "            '案件性质 Nature of the case',\n",
    "            '主要违法事实 Main illegal facts',\n",
    "            '相关法律依据及税务处理处罚 Relevant legal basis and tax treatment and punishment',\n",
    "            '注册地址 Registration address',\n",
    "            '数据来源 Data sources'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    df_event_gov_procure_illegal = df_event3.loc[bool_event_gov_procure_illegal, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '企业地址 Business address',\n",
    "            '不良行为的具体情形 Specific circumstances of the bad behavior',\n",
    "            '处罚依据 Penalty basis', '处罚结果 Penalty result', \n",
    "            '记录日期 Record date',\n",
    "            '登记地点 Registration location', \n",
    "            '处罚截止日期 Penalty deadline',\n",
    "            '数据来源 Data sources'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    df_event_overload_transport_illegal = df_event3.loc[bool_event_overload_transport_illegal, [\n",
    "            '统一社会信用代码 Unified Social Credit Identifier', \n",
    "            'company_name',\n",
    "            '入库时间 Registration time',\n",
    "            '车牌号 License number',\n",
    "            '营运证号 Operation certificate number', '道路运输证号 Road transport certificate number', \n",
    "            '批次 Batch',\n",
    "            '失信行为 Breach of trust',\n",
    "            '数据来源 Data sources'\n",
    "    ]].reset_index(drop=True)\n",
    "    \n",
    "    # year\n",
    "    \n",
    "    # dishonest person\n",
    "    df_event_dishonest_person = df_event_dishonest_person.pipe(\n",
    "        extract_year, '立案时间 Date of Filing', 'case_').pipe(\n",
    "        extract_year, '发布时间 Date of Issue', 'issue_')\n",
    "    \n",
    "    # gov\n",
    "    if df_event_gov_procure_illegal.shape[0] > 0:\n",
    "        df_event_gov_procure_illegal = df_event_gov_procure_illegal.pipe(\n",
    "            extract_year, '记录日期 Record date', 'start_').pipe(\n",
    "            extract_year, '处罚截止日期 Penalty deadline', 'end_')\n",
    "    \n",
    "    if df_event_overload_transport_illegal.shape[0] > 0:\n",
    "        df_event_overload_transport_illegal = df_event_overload_transport_illegal.pipe(\n",
    "            extract_year, '入库时间 Registration time', 'start_')\n",
    "    \n",
    "    if df_event_tax_blacklist.shape[0] > 0:\n",
    "        df_event_tax_blacklist = df_event_tax_blacklist.pipe(\n",
    "            extract_start_year, '主要违法事实 Main illegal facts')\n",
    "    \n",
    "    if df_event_safety_production.shape[0] > 0:\n",
    "        df_event_safety_production = df_event_safety_production.pipe(\n",
    "            extract_start_year, '失信行为简况 Default behavior profile')\n",
    "\n",
    "    print('saving...')\n",
    "    df_event_dishonest_person.to_excel(os.path.join(sub_event_path, '31_event_dishonest_person.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_safety_production.to_excel(os.path.join(sub_event_path, '32_event_safety_production.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_tax_blacklist.to_excel(os.path.join(sub_event_path, '33_event_tax_blacklist.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_gov_procure_illegal.to_excel(os.path.join(sub_event_path, '34_event_gov_procure_illegal.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_overload_transport_illegal.to_excel(os.path.join(sub_event_path, '35_event_overload_transport_illegal.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "\n",
    "    return df_event_dishonest_person, df_event_safety_production, df_event_tax_blacklist, df_event_gov_procure_illegal, df_event_overload_transport_illegal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53358b7",
   "metadata": {},
   "source": [
    "## 2.4 event4 clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88c87e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_event4(df_event4):\n",
    "    \"\"\"\n",
    "    Clean event4.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event4 : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    df_event4 = df_event4.pipe(\n",
    "        extract_year, '设定日期 Establishment date', 'start_')\n",
    "    \n",
    "    print('saving...')\n",
    "    df_event4[['统一社会信用代码 Unified Social Credit Identifier', 'company_name',\n",
    "           '列入经营异常名录原因类型名称 Reason type for listing in Operational abnormality',\n",
    "           'start_date', '列入决定机关名称 Listing decision authority name',\n",
    "           '数据来源 Data sources', 'start_year']].to_excel(os.path.join(sub_event_path, '4_event_abnormal_operations.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    \n",
    "    return df_event4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d963634",
   "metadata": {},
   "source": [
    "## 2.5 Split event5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11218dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_event5(df_event5):\n",
    "    \"\"\"\n",
    "    Split event5 into 2 types of credit commitments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_event5 : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_event_implementation : DataFrame\n",
    "    df_event_public : DataFrame\n",
    "    \"\"\"\n",
    "    bool_event_implementation = df_event5['承诺类型 Commitment type'].notna()\n",
    "\n",
    "    bool_event_public = df_event5['经办人 Manager'].notna()\n",
    "\n",
    "    check_equal(df_event5, [bool_event_implementation, bool_event_public])\n",
    "    \n",
    "    # split\n",
    "    df_event_implementation = df_event5.loc[bool_event_implementation, [\n",
    "        '统一社会信用代码 Unified Social Credit Identifier', 'company_name',\n",
    "        '承诺类型 Commitment type', '承诺事由 Commitment reason',\n",
    "           '做出承诺日期 Commitment date', '承诺受理单位 Commitment processing unit'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    df_event_public = df_event5.loc[bool_event_public, [\n",
    "        '统一社会信用代码 Unified Social Credit Identifier', 'company_name',\n",
    "        '信用承诺事项 Credit Commitment Matters',\n",
    "        '做出信用承诺时间 Time to make a credit commitment'\n",
    "    ]].reset_index(drop=True)\n",
    "    \n",
    "    # duplicates\n",
    "    results = check_duplicates([df_event_implementation, df_event_public])\n",
    "    \n",
    "    df_event_implementation.drop_duplicates(inplace=True)\n",
    "    df_event_public.drop_duplicates(inplace=True)\n",
    "    \n",
    "    df_event_implementation = df_event_implementation.pipe(\n",
    "        extract_year, '做出承诺日期 Commitment date', 'start_')\n",
    "    df_event_public = df_event_public.pipe(\n",
    "        extract_year, '做出信用承诺时间 Time to make a credit commitment', 'start_')\n",
    "    \n",
    "    # add theme\n",
    "    #df_event_implementation = add_commitment_themes(df_event_implementation)\n",
    "    \n",
    "    print('saving...')\n",
    "    df_event_implementation.to_excel(os.path.join(sub_event_path, '51_event_commitment_implementation.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    df_event_public.to_excel(os.path.join(sub_event_path, '52_event_commitment_public.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    \n",
    "    return df_event_implementation, df_event_public\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8be45",
   "metadata": {},
   "source": [
    "## final function to run all the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af516857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_events():\n",
    "    \n",
    "    \"\"\"\n",
    "    Split and pre-process events and metadata(organ types & region info).\n",
    "    \"\"\"   \n",
    "    df_meta, df_event1, df_event2, df_event3, df_event4, df_event5 = merge_companies()\n",
    "\n",
    "    df_meta = df_meta.pipe(extract_organ_types).pipe(add_regions)\n",
    "\n",
    "    # TODO:mannully fill out the 40 NA region info, since there is no way to identify the region info.(the meta in processed_data)\n",
    "\n",
    "    # delete 0 number columns \n",
    "    df_meta.drop(['司法判决 Judicial decision', '信用评价 Credit assessment', '其他信息 Other information'], axis=1, inplace=True)\n",
    "\n",
    "    # check distributions\n",
    "    draw_distribution_pie(df_meta.organ_type)\n",
    "    draw_distribution_pie(df_meta.province)\n",
    "    draw_distribution_pie(df_meta.level)\n",
    "\n",
    "    print('split event1...')\n",
    "    df_event_permit, df_event_penalty = split_event1(df_event1)\n",
    "\n",
    "    print('split event2...')\n",
    "    df_event_custom, df_event_a_taxpayer, df_event_highway, df_event_transportation = split_event2(df_event2)\n",
    "\n",
    "    # add lastest evaluation year of A taxpayer\n",
    "    df_year = df_event_a_taxpayer.groupby('统一社会信用代码 Unified Social Credit Identifier', as_index=False).max('评价年度 Evaluation year').rename(columns={\n",
    "            '统一社会信用代码 Unified Social Credit Identifier': '统一社会信用代码 Unified Social Credit Identifier, USCI',\n",
    "            '评价年度 Evaluation year': 'Latest evaluation year for A taxpayer'})\n",
    "    df_meta = df_meta.merge(df_year, how='left')\n",
    "\n",
    "    print('split event3...')\n",
    "    df_event_dishonest_person, df_event_safety_production, df_event_tax_blacklist, \\\n",
    "    df_event_gov_procure_illegal, df_event_overload_transport_illegal = split_event3(df_event3)\n",
    "\n",
    "    print('clean event4...')\n",
    "    df_event4 = split_event4(df_event4)\n",
    "\n",
    "    print('split event5...')\n",
    "    df_event_implementation, df_event_public = split_event5(df_event5)\n",
    "\n",
    "    # add id\n",
    "    df_meta = df_meta.reset_index().rename(columns={'index': 'ID'})\n",
    "    \n",
    "    # save a clean version for data transformation, leave the minimum columns\n",
    "    df_meta['foundation_year'] = df_meta['成立日期 Date of Foundation'].str[:4]\n",
    "    \n",
    "    df_meta['foundation_year'] = df_meta['foundation_year'].fillna(s_year) # TODO: 476 companies without foundation year.\n",
    "    \n",
    "    print('save meta...')\n",
    "    df_meta.to_excel(os.path.join(sub_event_path, 'metadata.xlsx'), index=False, freeze_panes=(1, 3))\n",
    "    \n",
    "    df_meta = df_meta[['ID', \n",
    "                 '机构名称 Institution name',\n",
    "                 '企业类型 Corporate type',\n",
    "                 'level', 'region', 'province', 'M_D', 'organ_type',\n",
    "                 'foundation_year',\n",
    "                 '成立日期 Date of Foundation',\n",
    "                 'Latest evaluation year for A taxpayer',\n",
    "                 'green', 'red', 'black', 'grey',\n",
    "                ]].rename(columns={'机构名称 Institution name': 'company_name',\n",
    "                                  '成立日期 Date of Foundation': 'foundation_date',\n",
    "                                  '企业类型 Corporate type': 'corporate_type'})\n",
    "\n",
    "    print('save clean meta...')\n",
    "    df_meta.to_excel(os.path.join(processed_data_path, 'metadata.xlsx'), index=False, freeze_panes=(1, 2))\n",
    "    \n",
    "    # save a excel with only company_name and ID\n",
    "    print('save ID-company...')\n",
    "    df_meta[['ID', 'company_name']].to_excel(os.path.join(attri_path, 'company_ids.xlsx'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701b9b9",
   "metadata": {},
   "source": [
    "split_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d25729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
